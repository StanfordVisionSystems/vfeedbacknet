#!/usr/bin/env python3

import asyncio
import argparse
import logging
import csv
import os
import sys
import timeit
import random

import numpy as np
import multiprocessing as mp
import sharedmem as sm
from concurrent.futures import ThreadPoolExecutor

import keras
import tensorflow as tf
from tensorflow.python.client import device_lib

import PIL
from PIL import Image

import vfeedbacknet as v

logging.basicConfig(level=logging.INFO)

def pool_init(shared_mem_):
    global shared_mem
    shared_mem = shared_mem_

def prepare_video(args):
    data_root, video_path, video_width, video_height, video_length, video_downsample_ratio, video_index, batch_size = args

    video_mem = np.frombuffer(shared_mem, np.ctypeslib.ctypes.c_float)
    video_mem = video_mem.reshape((batch_size, video_length, video_height, video_width))

    pathgen = lambda x : os.path.join(data_root, str(video_path), x)
    frames = sorted( os.listdir(pathgen('')) )

    num_frames = min(len(frames)//video_downsample_ratio, video_length)
    assert(num_frames != 0)
    
    video_mem[video_index,:,:,:] = 0 # make grey
    for i in range(num_frames):
        image_idx = video_downsample_ratio*i
        image = Image.open(pathgen(frames[image_idx])) # in RGB order by default
        image = image.convert('L') # convert to YUV and grab Y-component
        image = np.asarray(image.resize((video_width, video_height), PIL.Image.BICUBIC), dtype=np.float32)
        video_mem[video_index,i,:,:] = (image / 128) - 1 # squash to interval (-1, 1)

    return { 'num_frames' : num_frames, 'video_path' : video_path }

def load_videos(pool, data_root, data_labels, video_paths, video_width, video_height, video_length, video_downsample_ratio, batch_size, shared_mem):

    prepare_video_jobs = [ (data_root, video_paths[i], video_width, video_height, video_length, video_downsample_ratio, i, batch_size) for i in range(batch_size) ]
    prepared_videos = pool.map(prepare_video, prepare_video_jobs)

    video_numframes = np.zeros((batch_size,), dtype=np.int32)
    video_labelnums = np.zeros((batch_size,), dtype=np.int32)

    for i in range(batch_size):
        video_numframes[i] = prepared_videos[i]['num_frames']
        video_labelnums[i] = data_labels[ prepared_videos[i]['video_path'] ]

    video_mem = np.frombuffer(shared_mem, np.ctypeslib.ctypes.c_float)
    video_mem = video_mem.reshape((batch_size, video_length, video_height, video_width))

    batch = {
        'num_videos' : batch_size,
        'video_rawframes' : video_mem,
        'video_numframes' : video_numframes,
        'video_labelnums' : video_labelnums,
    }

    return batch

def main(args):
    logging.info('loading input files')
    with open(args.label_file, 'r') as csvfile:
        reader = csv.reader(csvfile, delimiter=' ')
        labels_num2str = [ item[1] for item in reader ]
        labels_str2num =  { label : idx  for idx,label in zip(range(len(labels_num2str)), labels_num2str) }
        assert(len(labels_num2str) == 101) # there should be 101 classes in the UCF-101 dataset
        
    with open(args.data_file, 'r') as csvfile:
        reader = csv.reader(csvfile, delimiter=' ')
        data_video_paths = [ item[0].split('.')[0] for item in reader ] 
        data_labels = { item : labels_str2num[item.split('/')[0]] for item in data_video_paths } 
        random.seed(123) # shuffle with this particular seed to help with debugging
        random.shuffle(data_video_paths)

        # force overfitting
        args.prefetch_batch_size = 16
        data_video_paths = data_video_paths[:args.prefetch_batch_size]
        
    # allocate shared memory up front
    logging.info('allocating memory')
    logging.debug('begin allocate memory buffers')
    t1 = timeit.default_timer()
    shared_mem = sm.empty(args.prefetch_batch_size*args.video_length*args.video_height*args.video_width, dtype='f')
    zeros = np.zeros((args.eval_batch_size,))
    t2 = timeit.default_timer()
    logging.debug('done! (allocate memory buffers: {})'.format(t2-t1))

    round2num = lambda x, num: num * (x // num)
    num_data_videos = round2num(len(data_video_paths), args.prefetch_batch_size)
    num_batch_videos = round2num(args.prefetch_batch_size, args.eval_batch_size)
    assert(num_batch_videos == args.prefetch_batch_size) # make `prefetch_batch_size` a multiple of `eval_batch_size`

    # get batch of data and train
    logging.info('begin processing data')
    with mp.Pool(processes=mp.cpu_count(), initializer=pool_init, initargs=(shared_mem,)) as pool:
        with ThreadPoolExecutor(max_workers=2) as executor:
            video_batch = None
            validation_count = 0
            train_batch_count = 0

            with tf.Session() as sess:
                logging.info('load model')
                saver = tf.train.import_meta_graph(args.checkpoint_prefix+'.meta')
                saver.restore(sess, args.checkpoint_prefix)

                logging.info('process data')
                cum_data_correct_predictions = 0
                cum_data_loss = 0

                for batch_base_idx in range(0, num_data_videos, args.prefetch_batch_size):
                    video_paths_batch = data_video_paths[batch_base_idx:batch_base_idx+args.prefetch_batch_size]
                    validation_count += 1

                    logging.debug('begin load videos)')
                    t1 = timeit.default_timer()
                    video_batch = load_videos(pool, args.data_root, data_labels, video_paths_batch, args.video_width, args.video_height, args.video_length, args.video_downsample_ratio, args.prefetch_batch_size, shared_mem)
                    t2 = timeit.default_timer()
                    logging.debug('done! (load videos): {})'.format(t2-t1))

                    # print('out a video to make sure things are working')
                    # for i in range(40):
                    #     frame = (128*(video_batch['video_rawframes'][0,i,:,:,:] + 1)).astype(dtype=np.uint8)
                    #     im = Image.fromarray(frame)
                    #     im.save('/home/jemmons/frame{}.jpg'.format(i))

                    assert(not np.any(np.isnan(video_batch['video_rawframes'])))

                    ########################################################
                    # evaluate model
                    ########################################################
                    logging.debug('begin train batch')
                    t1 = timeit.default_timer()
                    for i in range(0, num_batch_videos, args.eval_batch_size):
                        begin = i
                        end = i + args.eval_batch_size
                        batch_labels = keras.utils.to_categorical(video_batch['video_labelnums'][begin:end], len(labels_num2str))
                        
                        _total_loss_val, _loss_vals, _predict_vals = sess.run(['total_loss:0', 'losses:0', 'predictions:0'],
                                                                              feed_dict={'x_input:0' : video_batch['video_rawframes'][begin:end,:,:,:],
                                                                                         'x_length:0' : video_batch['video_numframes'][begin:end],
                                                                                         'y_label:0' : batch_labels,
                                                                                         'y_zeros:0' : zeros})
                        predict_analysis = []
                        for video_idx in range(args.eval_batch_size):
                            video_labelnum = video_batch['video_labelnums'][begin:end][video_idx]
                            video_labelstr = labels_num2str[video_labelnum]
                            video_length = video_batch['video_numframes'][video_idx]
                            
                            fvec = []
                            fmax = []
                            fsum = []
                            floss = []
                            for frame_idx in range(video_batch['video_numframes'][video_idx]):
                                predict = _predict_vals[video_idx, frame_idx, :]
                                fvec.append(predict)
                                fsum.append(sum(predict))
                                fmax.append(np.argmax(predict))
                                
                                floss.append(_loss_vals[video_idx, :])
                                    
                            predict_analysis.append({
                                'frame_labelvec' : fvec,
                                'frame_labelmax' : fmax,
                                'frame_probsum' : fsum,
                                'frame_loss' : floss,
                                'video_length' : video_length,
                                'video_labelnum' : video_labelnum,
                                'video_labelstr' : video_labelstr
                            })

                        for analysis in predict_analysis:
                            for a in analysis['frame_probsum']:
                                assert( abs(a - 1) < 0.00001 )
                                
                            fmax_str = list(map(lambda x: str(x).zfill(2), analysis['frame_labelmax']))
                            tl = str(analysis['video_labelnum']).zfill(2)
                            logging.debug('{} true_label,prediction: {},{} {}'.format('T' if tl==fmax_str[-1] else 'F', tl, fmax_str[-1], fmax_str))
                            #logging.debug('{}'.format(analysis['frame_loss']))
                            
                        assert(not np.isnan(_total_loss_val))
                        assert(not np.any(np.isnan(_predict_vals)))
                        
                        predicted_vals = np.asarray([ p['frame_labelmax'][-1] for p in predict_analysis ])
                        correct_predictions = sum(video_batch['video_labelnums'][begin:end] == predicted_vals)
                        
                        cum_data_correct_predictions += correct_predictions
                        cum_data_loss += _total_loss_val
                        
                        # assert(not np.isnan(loss_val))
                        # assert(not np.any(np.isnan(predict_vals)))
                        # predict_vals = np.vstack([predict_vals[n-1,m,:] for n,m in zip(video_batch['video_numframes'][begin:end],range(args.eval_batch_size))])

                        # cum_data_correct_predictions += sum(video_batch['video_labelnums'][begin:end] == np.argmax(predict_vals, axis=1))
                        # cum_data_loss += loss_val

                        logging.info('BATCH\taccuracy (top-1): {}'.format(correct_predictions / args.eval_batch_size))
                        logging.info('BATCH\tloss: {}'.format(_total_loss_val))
                        logging.info('BATCH\tnum_examples: {}'.format(args.eval_batch_size))
                        train_batch_count += 1

                        # logging.info('BATCH\taccuracy (top-1): {}'.format(sum(video_batch['video_labelnums'][begin:end] == np.argmax(predict_vals, axis=1)) / args.eval_batch_size))
                        # logging.info('BATCH\tloss: {}'.format(loss_val))
                        # logging.info('BATCH\tnum examples: {}'.format(args.eval_batch_size))
                        # train_batch_count += 1

                    t2 = timeit.default_timer()
                    logging.debug('done! (train batch: {})'.format(t2-t1))

            # print out the final accuracy and loss
            sys.stdout.write('final loss: {}\n'.format(cum_data_loss))
            sys.stdout.write('final accuracy: {}\n'.format(cum_data_correct_predictions/num_data_videos))
            sys.stdout.flush()

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='test the feedbacknet for the UCF-101 data set')

    # dataset parameters
    parser.add_argument('label_file', type=str, nargs=None,
                        help='labels file')

    parser.add_argument('data_file', type=str, nargs=None,
                        help='data label file')

    parser.add_argument('data_root', type=str, nargs=None,
                        help='root of UCF-101 dataset)')

    parser.add_argument('checkpoint_prefix', type=str, nargs=None,
                        help='path prefix to the model {metadata,data} to load')

    # tuning parameters
    parser.add_argument('--video_width', type=int, nargs=None,
                        help='the width to rescale all videos (official run: XXX for UCF-101)',
                        default=96)

    parser.add_argument('--video_height', type=int, nargs=None,
                        help='the height to rescale all videos (official run: XXXX for UCF-101)',
                        default=96)

    parser.add_argument('--video_length', type=int, nargs=None,
                        help='the num frames to truncate all videos (official run: XXXX for UCF-101)',
                        default=40)

    parser.add_argument('--video_downsample_ratio', type=int, nargs=None,
                        help='the num frames to truncate all videos (official run: XXXX for UCF-101)',
                        default=5)
    
    parser.add_argument('--eval_batch_size', type=int, nargs=None,
                        help='number of videos to eval at a time (official run: XXXX for UCF-101)',
                        default=16)

    parser.add_argument('--prefetch_batch_size', type=int, nargs=None,
                        help='number of videos to prefetch (official run: XXXX for UCF-101)',
                        default=1024)

    args = parser.parse_args()
    main(args)

