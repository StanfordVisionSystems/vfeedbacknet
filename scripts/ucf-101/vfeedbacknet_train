#!/usr/bin/env python3

import asyncio
import argparse
import datetime
import logging
import csv
import os
import sys
import timeit
import random

import numpy as np
import multiprocessing as mp
import sharedmem as sm

import PIL
from PIL import Image

import keras
import tensorflow as tf
from tensorflow.python.client import device_lib

import vfeedbacknet as v

#logging.basicConfig(level=logging.INFO)
logging.basicConfig(level=logging.DEBUG)

def pool_init(shared_mem_):
    global shared_mem
    shared_mem = shared_mem_

def prepare_video(args):
    data_root, video_path, video_width, video_height, video_length, video_downsample_ratio, video_index, batch_size = args

    video_mem = np.frombuffer(shared_mem, np.ctypeslib.ctypes.c_float)
    video_mem = video_mem.reshape((batch_size, video_length, video_height, video_width))

    pathgen = lambda x : os.path.join(data_root, str(video_path), x)
    frames = sorted( os.listdir(pathgen('')) )

    num_frames = min(len(frames)//video_downsample_ratio, video_length)
    assert(num_frames != 0)
    
    video_mem[video_index,:,:,:] = 0 # make grey
    for i in range(num_frames):
        image_idx = video_downsample_ratio*i
        image = Image.open(pathgen(frames[image_idx])) # in RGB order by default
        image = image.convert('L') # convert to YUV and grab Y-component
        image = np.asarray(image.resize((video_width, video_height), PIL.Image.BICUBIC), dtype=np.float32)
        video_mem[video_index,i,:,:] = (image / 128) - 1 # squash to interval (-1, 1)

    return { 'num_frames' : num_frames, 'video_path' : video_path }

def load_videos(pool, data_root, data_labels, video_paths, video_width, video_height, video_length, video_downsample_ratio, batch_size, shared_mem):

    prepare_video_jobs = [ (data_root, video_paths[i], video_width, video_height, video_length, video_downsample_ratio, i, batch_size) for i in range(batch_size) ]
    prepared_videos = pool.map(prepare_video, prepare_video_jobs)

    video_numframes = np.zeros((batch_size,), dtype=np.int32)
    video_labelnums = np.zeros((batch_size,), dtype=np.int32)

    for i in range(batch_size):
        video_numframes[i] = prepared_videos[i]['num_frames']
        video_labelnums[i] = data_labels[ prepared_videos[i]['video_path'] ]

    video_mem = np.frombuffer(shared_mem, np.ctypeslib.ctypes.c_float)
    video_mem = video_mem.reshape((batch_size, video_length, video_height, video_width))

    batch = {
        'num_videos' : batch_size,
        'video_rawframes' : video_mem,
        'video_numframes' : video_numframes,
        'video_labelnums' : video_labelnums,
    }

    return batch

def main(args):
    ############################################################################
    # open and parse the data and label files
    ############################################################################
    logging.info('loading input files')
    with open(args.label_file, 'r') as csvfile:
        reader = csv.reader(csvfile, delimiter=' ')
        labels_num2str = [ item[1] for item in reader ]
        labels_str2num =  { label : idx  for idx,label in zip(range(len(labels_num2str)), labels_num2str) }
        
    with open(args.validation_file, 'r') as csvfile:
        reader = csv.reader(csvfile, delimiter=' ')
        validation_labels = { item[0].split('.')[0] : labels_str2num[item[0].split('/')[0]] for item in reader }; 
        validation_video_paths = list(validation_labels.keys())

        if args.overfit:
            logging.info('OVERFITTING!')
            args.prefetch_batch_size = 16
            validation_video_paths = validation_video_paths[:args.prefetch_batch_size] # force overfitting
        elif args.overfitt:
            logging.info('OVERFITTING!')
            args.prefetch_batch_size = 1024
            validation_video_paths = validation_video_paths[:args.prefetch_batch_size] # force overfitting
            
    with open(args.data_file, 'r') as csvfile:
        reader = csv.reader(csvfile, delimiter=';')
        data_labels = { item[0].split('.')[0] : labels_str2num[item[0].split('/')[0]] for item in reader }; 
        data_video_paths = list(data_labels.keys())

        if args.overfit:
            logging.info('OVERFITTING!')
            data_video_paths = data_video_paths[:args.prefetch_batch_size] # force overfitting
        elif args.overfitt:
            logging.info('OVERFITTING!')
            data_video_paths = data_video_paths[:args.prefetch_batch_size] # force overfitting
            
    # local_device_protos = device_lib.list_local_devices()
    # available_gpus = [x.name for x in local_device_protos if x.device_type == 'GPU']
    # num_gpus = len(available_gpus)
    # logging.debug('there are {} available gpus: {}'.format(num_gpus, available_gpus))

    ############################################################################
    # build model
    ############################################################################
    logging.info('building model: {}'.format(args.model_name))
    x_input = tf.placeholder(tf.float32, [None, args.video_length, args.video_height, args.video_width], name='x_input')
    x_length = tf.placeholder(tf.int32, [None,], name='x_length')

    assert(len(labels_num2str) == 101) # there should be 101 classes in the 20BN dataset
    y_label = tf.placeholder(tf.float32, [None, len(labels_num2str)], name='y_label')

    y_zeros = tf.placeholder(tf.float32, [None,],  name='y_zeros')
    zeros = np.zeros((args.train_batch_size,))

    losses, total_loss, predictions = eval('v.{}(args.video_length, args.video_width, args.video_height, len(labels_num2str), x_input, x_length, y_label, y_zeros)'.format(args.model_name))

    step = tf.Variable(0, trainable=False)
    learning_rate = tf.maximum(tf.train.exponential_decay(args.learning_rate_init, step, args.prefetch_batch_size//args.train_batch_size, args.learning_rate_decay), args.learning_rate_min)
    train_step = tf.train.AdadeltaOptimizer(learning_rate).minimize(total_loss, global_step=step)

    # create saver
    saver = tf.train.Saver()

    ############################################################################
    # allocate shared memory up front 
    ############################################################################
    logging.info('allocating memory')
    logging.debug('begin allocate memory buffers')
    t1 = timeit.default_timer()
    shared_mem = sm.empty(args.prefetch_batch_size*args.video_length*args.video_height*args.video_width, dtype='f')
    t2 = timeit.default_timer()
    logging.debug('done! (allocate memory buffers: {})'.format(t2-t1))

    round2num = lambda x, num: num * (x // num)
    num_data_videos = round2num(len(data_video_paths), args.prefetch_batch_size)
    num_validation_videos = round2num(len(validation_video_paths), args.prefetch_batch_size)
    num_batch_videos = round2num(args.prefetch_batch_size, args.train_batch_size)
    assert(num_batch_videos == args.prefetch_batch_size) # make `prefetch_batch_size` a multiple of `train_batch_size`

    ############################################################################
    # begin the training process
    ############################################################################
    logging.info('begin training')
    with open(args.train_progress_csv, 'w') as logfile:
        if args.overfit or args.overfitt:
            logfile.write('# OVERFITTING! OVERFITTING! OVERFITTING! \n')
        logfile.write('# {}\n'.format(' '.join(sys.argv)))
        logfile.write('# {}\n'.format(args.model_name))
        logfile.write('# {}\n'.format(args.checkpoint_dir))
        logfile.write('# {}\n'.format(datetime.datetime.now()))
        logfile.write('# model_name:{} data_size:{} validation_size:{} prefetch_batch_size:{} train_batch_size:{} initial_learning_rate:{} learning_rate_decay:{} minimum_learning_rate:{} overfit:{}\n'.format(args.model_name, num_data_videos, num_validation_videos, args.prefetch_batch_size, args.train_batch_size, args.learning_rate_init, args.learning_rate_decay, args.learning_rate_min, args.overfit or args.overfitt))
        logfile.write('# video_width:{} video_height:{} video_length:{} video_downsample_ratio:{} \n'.format(args.video_width, args.video_height, args.video_length, args.video_downsample_ratio))
        logfile.write('batch_num, learning_rate, train_acc, train_loss, valid_acc, valid_loss\n')
        logfile.flush()

        with mp.Pool(processes=mp.cpu_count(), initializer=pool_init, initargs=(shared_mem,)) as pool:
            validation_count = 0
            train_batch_count = 0
            train_minibatch_count = 0

            #with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:
            with tf.Session() as sess:
                sess.run(tf.global_variables_initializer())

                if args.continue_training:
                    saver.restore(sess, tf.train.latest_checkpoint(args.checkpoint_dir))
                
                for epoch in range(args.num_epoch):
                    video_paths = data_video_paths.copy()
                    random.shuffle(video_paths)

                    ############################################################
                    # loop over all the videos in a random order
                    ############################################################
                    for batch_base_idx in range(0, num_data_videos, args.prefetch_batch_size):
                        video_paths_batch = video_paths[batch_base_idx:batch_base_idx+args.prefetch_batch_size]

                        logging.debug('begin load videos')
                        t1 = timeit.default_timer() 
                        video_batch = load_videos(pool, args.data_root, data_labels, video_paths_batch, args.video_width, args.video_height, args.video_length, args.video_downsample_ratio, args.prefetch_batch_size, shared_mem)
                        t2 = timeit.default_timer()
                        logging.debug('done! (load videos): {})'.format(t2-t1))

                        # print('out a video to make sure things are working')
                        # print(video_batch['video_labelnums'][0], labels_num2str[video_batch['video_labelnums'][0]])
                        # for i in range(args.video_length):
                        #     frame = (128*(video_batch['video_rawframes'][0,i,:,:] + 1)).astype(dtype=np.uint8)
                        #     im = Image.fromarray(frame)
                        #     im.save('/home/jemmons/frame{}.jpg'.format(i))
                        # return 
                            
                        assert(not np.any(np.isnan(video_batch['video_rawframes'])))

                        ########################################################
                        # train model
                        ########################################################
                        logging.debug('begin train batch')
                        t1 = timeit.default_timer()

                        cum_data_loss = 0
                        cum_data_correct_predictions = 0

                        for i in range(0, num_batch_videos, args.train_batch_size):
                            begin = i
                            end = i + args.train_batch_size
                            batch_labels = keras.utils.to_categorical(video_batch['video_labelnums'][begin:end], len(labels_num2str))
                                                                      
                            _, _learn_rate, _total_loss_val, _loss_vals, _predict_vals = sess.run([train_step, learning_rate, total_loss, losses, predictions],
                                                                                             feed_dict={x_input : video_batch['video_rawframes'][begin:end,:,:],
                                                                                                        x_length : video_batch['video_numframes'][begin:end],
                                                                                                        y_label : batch_labels,
                                                                                                        y_zeros : zeros})
                            predict_analysis = []
                            for video_idx in range(args.train_batch_size):
                                video_labelnum = video_batch['video_labelnums'][begin:end][video_idx]
                                video_labelstr = labels_num2str[video_labelnum]
                                video_length = video_batch['video_numframes'][video_idx]
                                
                                fvec = []
                                fmax = []
                                fsum = []
                                floss = []
                                for frame_idx in range(video_batch['video_numframes'][video_idx]):
                                    predict = _predict_vals[video_idx, frame_idx, :]
                                    fvec.append(predict)
                                    fsum.append(sum(predict))
                                    fmax.append(np.argmax(predict))

                                    floss.append(_loss_vals[video_idx, :])
                                    
                                predict_analysis.append({
                                    'frame_labelvec' : fvec,
                                    'frame_labelmax' : fmax,
                                    'frame_probsum' : fsum,
                                    'frame_loss' : floss,
                                    'video_length' : video_length,
                                    'video_labelnum' : video_labelnum,
                                    'video_labelstr' : video_labelstr
                                })

                            for analysis in predict_analysis:
                                for a in analysis['frame_probsum']:
                                    assert( abs(a - 1) < 0.00001 )

                                fmax_str = list(map(lambda x: str(x).zfill(2), analysis['frame_labelmax']))
                                tl = str(analysis['video_labelnum']).zfill(2)
                                logging.debug('{} true_label,prediction: {},{} {}'.format('T' if tl==fmax_str[-1] else 'F', tl, fmax_str[-1], fmax_str))
                                #logging.debug('{}'.format(analysis['frame_loss']))

                            assert(not np.isnan(_total_loss_val))
                            assert(not np.any(np.isnan(_predict_vals)))

                            predicted_vals = np.asarray([ p['frame_labelmax'][-1] for p in predict_analysis ])
                            correct_predictions = sum(video_batch['video_labelnums'][begin:end] == predicted_vals)
                            
                            cum_data_correct_predictions += correct_predictions
                            cum_data_loss += _total_loss_val

                            logging.info('TRAINING BATCH\taccuracy (top-1): {}'.format(correct_predictions / args.train_batch_size))
                            logging.info('TRAINING BATCH\tloss: {}'.format(_total_loss_val))
                            logging.info('TRAINING BATCH\tlearning_rate: {}'.format(_learn_rate))
                            logging.info('TRAINING BATCH\tnum_examples: {}'.format(args.train_batch_size))
                            train_minibatch_count += 1

                        logfile.write('{}, {}, {}, {}, , \n'.format(train_batch_count, _learn_rate, cum_data_correct_predictions/args.prefetch_batch_size, cum_data_loss))
                        logfile.flush()

                        t2 = timeit.default_timer()
                        logging.debug('done! (train batch: {})'.format(t2-t1))

                        ########################################################
                        # save the model and compute validation accuracy/loss
                        ########################################################
                        if validation_count == args.validation_interval:
                            validation_count = 0

                            ####################################################
                            # save the model
                            ####################################################
                            logging.info('SAVING MODEL to disk')
                            t1 = timeit.default_timer() 
                            checkpoint_path = os.path.join(args.checkpoint_dir, 'vfeedbacknet-epoch{}-batch{}'.format(epoch,train_minibatch_count))
                            saver.save(sess, checkpoint_path)
                            t2 = timeit.default_timer()
                            logging.info('SAVING MODEL done! (save model to disk): {})'.format(t2 - t1))

                            ####################################################
                            # compute validation accuracy/loss
                            ####################################################
                            logging.debug('begin validation accuracy computation')
                            t1 = timeit.default_timer() 

                            cum_validation_loss = 0
                            cum_validation_correct_predictions = 0

                            for valid_base_idx in range(0, num_validation_videos, args.prefetch_batch_size):
                                validation_batch = load_videos(pool, args.data_root, validation_labels, validation_video_paths[valid_base_idx:valid_base_idx+args.prefetch_batch_size], args.video_width, args.video_height, args.video_length, args.video_downsample_ratio, args.prefetch_batch_size, shared_mem)

                                for i in range(0, num_batch_videos, args.train_batch_size):
                                    begin = i
                                    end = i + args.train_batch_size

                                    learn_rate, total_loss_val, predict_vals = sess.run([learning_rate, total_loss, predictions],
                                                                      feed_dict={x_input : validation_batch['video_rawframes'][begin:end,:,:,:],
                                                                                 x_length : validation_batch['video_numframes'][begin:end],
                                                                                 y_label : keras.utils.to_categorical(validation_batch['video_labelnums'][begin:end],len(labels_num2str)),
                                                                                 y_zeros : zeros})

                                    assert(not np.isnan(total_loss_val))
                                    assert(not np.any(np.isnan(predict_vals)))

                                    predict_vals = np.vstack([predict_vals[n,m-1,:] for n,m in zip(range(args.train_batch_size),validation_batch['video_numframes'][begin:end])])

                                    cum_validation_correct_predictions += sum(validation_batch['video_labelnums'][begin:end] == np.argmax(predict_vals, axis=1))
                                    cum_validation_loss += total_loss_val

                                    logging.debug('VALIDATION batch accuracy: {}'.format(sum(validation_batch['video_labelnums'][begin:end] == np.argmax(predict_vals, axis=1)) / args.train_batch_size))
                                    logging.debug('VALIDATION batch loss: {}'.format(total_loss_val))

                            logfile.write('{}, {}, , , {}, {}\n'.format(train_batch_count, learn_rate, cum_validation_correct_predictions/num_validation_videos, cum_validation_loss))
                            logfile.flush()

                            logging.info('VALIDATION TOTAL\taccuracy (top-1): {}'.format(cum_validation_correct_predictions / num_validation_videos))
                            logging.info('VALIDATION TOTAL\tloss: {}'.format(cum_validation_loss))
                            logging.info('VALIDATION TOTAL\tnum_examples: {}'.format(num_validation_videos))

                            t2 = timeit.default_timer()
                            logging.debug('done! (validation accuracy computation): {})'.format(t2 - t1))

                            ####################################################
                            # restore model to the state prior to computing validation accuracy/loss
                            ####################################################
                            saver.restore(sess, checkpoint_path)

                        validation_count += 1
                        train_batch_count += 1

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='train the feedbacknet for the 20BN data set')

    # dataset parameters (all required)
    parser.add_argument('label_file', type=str, nargs=None,
                        help='UCF-101 labels file')

    parser.add_argument('validation_file', type=str, nargs=None,
                        help='UCF-101 validation label file')

    parser.add_argument('data_file', type=str, nargs=None,
                        help='UCF-101 data label file')

    parser.add_argument('data_root', type=str, nargs=None,
                        help='root of UCF-101 dataset (make sure to run extract_UCF-101_frames.sh)')

    parser.add_argument('model_name', type=str, nargs=None,
                        help='name of the model to train')

    parser.add_argument('checkpoint_dir', type=str, nargs=None,
                        help='location to store checkpoints during training')

    parser.add_argument('train_progress_csv', type=str, nargs=None,
                        help='csv file to record the training and validation error/loss')

    # tuning parameters (optional)
    parser.add_argument('--overfit',
                        help='overfit on a small chunk [64 data points] of data (official run: False for UCF-101)',
                        action='store_true',
                        default=False)

    parser.add_argument('--overfitt',
                        help='overfit on a larger chunk of data (official run: False for UCF-101)',
                        action='store_true',
                        default=False)

    parser.add_argument('--continue_training',
                        help='load the most recent set of weights and continue training (official run: False for UCF-101)',
                        action='store_true',
                        default=False)

    parser.add_argument('--video_width', type=int, nargs=None,
                        help='the width to rescale all videos (official run: 176 for UCF-101)',
                        default=96)

    parser.add_argument('--video_height', type=int, nargs=None,
                        help='the height to rescale all videos (official run: XXXX for UCF-101)',
                        default=96)

    parser.add_argument('--video_length', type=int, nargs=None,
                        help='the num frames to truncate all videos (official run: XXXX for UCF-101)',
                        default=40)
    
    parser.add_argument('--video_downsample_ratio', type=int, nargs=None,
                        help='the temporal downsampling ratio to use for all the videos (official run: XXXX for UCF-101)',
                        default=5)
    
    parser.add_argument('--num_epoch', type=int, nargs=None,
                        help='number of training epochs over the whole dataset (official run: XXXX for UCF-101)',
                        default=256000)

    parser.add_argument('--learning_rate_init', type=float, nargs=None,
                        help='initial vlaue for learning rate for the optimization (official run: XXXX for UCF-101)',
                        default=1e-2)

    parser.add_argument('--learning_rate_decay', type=float, nargs=None,
                        help='learning rate decay (per prefetch_batch) for the optimization (official run: XXXX for UCF-101)',
                        default=0.999)

    parser.add_argument('--learning_rate_min', type=float, nargs=None,
                        help='minimum learning rate for the optimization (official run: XXXX for UCF-101)',
                        default=1e-4)
    
    parser.add_argument('--train_batch_size', type=int, nargs=None,
                        help='training batch size; num videos per batch (official run: XXXX for UCF-101)',
                        default=16)

    parser.add_argument('--prefetch_batch_size', type=int, nargs=None,
                        help='number of videos to prefetch (official run: XXXX for UCF-101)',
                        default=1024)

    parser.add_argument('--validation_interval', type=int, nargs=None,
                        help='number of training batches to process before printing validation error (official run: XXXX for UCF-101)',
                        default=32)

    args = parser.parse_args()
    main(args)

