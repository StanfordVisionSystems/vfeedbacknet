#!/usr/bin/env python3

import asyncio
import argparse
import datetime
import logging
import csv
import os
import sys
import timeit
import random

import numpy as np
import scipy.stats as stats
import multiprocessing as mp
import sharedmem as sm

import keras
import tensorflow as tf
from tensorflow.python.client import device_lib

import vfeedbacknet as v
from vfeedbacknet import pool_init, prepare_video, load_videos 

#logging.basicConfig(level=logging.INFO)
logging.basicConfig(level=logging.DEBUG)

def main(args):
    ############################################################################
    # open and parse the data and label files
    ############################################################################
    logging.info('loading input files')
    assert args.ucf101 ^ args.twentybn, 'you must specify exactly one dataset to use for training (XOR)'

    with open(args.label_file, 'r') as csvfile:
        labels_num2str = None
        labels_str2num = None
        
        if args.ucf101:
            reader = csv.reader(csvfile, delimiter=' ')
            labels_num2str = [ item[1] for item in reader ]
            labels_str2num =  { label : idx  for idx,label in zip(range(len(labels_num2str)), labels_num2str) }
        elif args.twentybn:
            reader = csv.reader(csvfile, delimiter=';')
            labels_num2str = [ item[0].lower() for item in reader ]
            labels_str2num =  { label : idx  for idx,label in zip(range(len(labels_num2str)), labels_num2str) }
            
    with open(args.validation_file, 'r') as csvfile:
        validation_video_paths = None
        validation_labels = None

        if args.ucf101:
            reader = csv.reader(csvfile, delimiter=' ')
            validation_video_paths = [ item[0].split('.')[0] for item in reader ] 
            validation_labels = { item : labels_str2num[item.split('/')[0]] for item in validation_video_paths } 
        elif args.twentybn:
            reader = csv.reader(csvfile, delimiter=';')
            validation_labels = { int(item[0]) : labels_str2num[item[1].lower()] for item in reader }; 
            validation_video_paths = np.asarray( list(validation_labels.keys()) )
            
        random.seed(123) # shuffle with this particular seed to help with debugging
        random.shuffle(validation_video_paths)

        if args.overfit:
            logging.info('OVERFITTING!')
            args.prefetch_batch_size = 16
            validation_video_paths = validation_video_paths[:args.prefetch_batch_size] # force overfitting
        elif args.overfitt:
            logging.info('OVERFITTING!')
            args.prefetch_batch_size = 1024
            validation_video_paths = validation_video_paths[:args.prefetch_batch_size] # force overfitting
            
    with open(args.data_file, 'r') as csvfile:
        data_video_paths = None
        data_labels = None

        if args.ucf101:
            reader = csv.reader(csvfile, delimiter=' ')
            data_video_paths = [ item[0].split('.')[0] for item in reader ] 
            data_labels = { item : labels_str2num[item.split('/')[0]] for item in data_video_paths } 
        elif args.twentybn:
            reader = csv.reader(csvfile, delimiter=';')
            data_labels = { int(item[0]) : labels_str2num[item[1].lower()] for item in reader }; 
            data_video_paths = np.asarray( list(data_labels.keys()) )

        random.seed(123) # shuffle with this particular seed to help with debugging
        random.shuffle(data_video_paths)

        if args.overfit:
            logging.info('OVERFITTING!')
            data_video_paths = data_video_paths[:args.prefetch_batch_size] # force overfitting
        elif args.overfitt:
            logging.info('OVERFITTING!')
            data_video_paths = data_video_paths[:args.prefetch_batch_size] # force overfitting
            
    # local_device_protos = device_lib.list_local_devices()
    # available_gpus = [x.name for x in local_device_protos if x.device_type == 'GPU']
    # num_gpus = len(available_gpus)
    # logging.debug('there are {} available gpus: {}'.format(num_gpus, available_gpus))

    ############################################################################
    # build model
    ############################################################################
    logging.info('building model: {}'.format(args.model_name))
    sess = tf.Session()

    x_input = tf.placeholder(tf.float32,
                             [None, args.video_length,
                              args.video_height, args.video_width],
                             name='x_input')
    x_length = tf.placeholder(tf.int32, [None,], name='x_length')
    
    logging.debug('creating model with {} outputs'.format(len(labels_num2str)))
    y_label = tf.placeholder(tf.float32, [None, len(labels_num2str)], name='y_label')
    
    y_zeros = tf.placeholder(tf.float32, [None,],  name='y_zeros')
    zeros = np.zeros((args.train_batch_size,))

    model_generator_dict = {
        'vfeedbacknet_model1' : lambda : v.vfeedbacknet_model1.VFeedbackNetModel1(sess, len(labels_num2str), train_featurizer='NO', weights_filename='/home/jemmons/vfeedbacknet_base_weights.npz'),
        # legacy models below
        # 'NoFeedbackNetVgg16' : lambda : v.NoFeedbackNetVgg16(sess, '/home/jemmons/vgg16_weights.npz', num_classes=len(labels_num2str), fine_tune_vgg16=args.fine_tune_vgg),
        # 'NoFeedbackNetLSTMVgg16_2LSTM' : lambda : v.NoFeedbackNetLSTMVgg16_2LSTM(sess, '/home/jemmons/vgg16_weights.npz', num_classes=len(labels_num2str), fine_tune_vgg16=args.fine_tune_vgg),
        # 'NoFeedbackNetLSTMVgg16_reg0_5' : lambda : v.NoFeedbackNetLSTMVgg16_reg0_5(sess, '/home/jemmons/vgg16_weights.npz', num_classes=len(labels_num2str), fine_tune_vgg16=args.fine_tune_vgg),
        # 'NoFeedbackNetLSTMVgg16_short' : lambda : v.NoFeedbackNetLSTMVgg16_short(sess, '/home/jemmons/vgg16_weights.npz', num_classes=len(labels_num2str), fine_tune_vgg16=args.fine_tune_vgg),
        # 'NoFeedbackNetGRUVgg16_short' : lambda : v.NoFeedbackNetGRUVgg16_short(sess, '/home/jemmons/vgg16_weights.npz', num_classes=len(labels_num2str), fine_tune_vgg16=args.fine_tune_vgg),
        # 'NoFeedbackNetLSTMVgg16' : lambda : v.NoFeedbackNetLSTMVgg16(sess, '/home/jemmons/vgg16_weights.npz', num_classes=len(labels_num2str), fine_tune_vgg16=args.fine_tune_vgg),
        # 'NoFeedbackNetGRUVgg16' : lambda : v.NoFeedbackNetGRUVgg16(sess, '/home/jemmons/vgg16_weights.npz', num_classes=len(labels_num2str), fine_tune_vgg16=args.fine_tune_vgg),        
    }

    model = model_generator_dict[args.model_name]()
    logits = model(x_input, x_length)

    losses, total_loss, predictions = v.vfeedbacknet_lossfunctions.basic_loss_pred(logits,
                                                                                   x_length,
                                                                                   args.video_length,
                                                                                   y_label,
                                                                                   y_zeros,
                                                                                   last_loss_multiple=args.last_loss_multipler)

    step = tf.Variable(args.global_step_init if (args.global_step_init is not None) else 0, trainable=False, name='step')
    learning_rate = tf.maximum(tf.train.exponential_decay(args.learning_rate_init, step, args.prefetch_batch_size//args.train_batch_size, args.learning_rate_decay), args.learning_rate_min, name='learning_rate')
    optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate, name='optimizer')
    train_step = optimizer.minimize(total_loss, global_step=step)
    saver = tf.train.Saver()

    ############################################################################
    # allocate shared memory up front 
    ############################################################################
    logging.info('allocating memory')
    logging.debug('begin allocate memory buffers')
    t1 = timeit.default_timer()
    shared_mem = sm.empty(args.prefetch_batch_size*args.video_length*args.video_height*args.video_width, dtype='f')
    t2 = timeit.default_timer()
    logging.debug('done! (allocate memory buffers: {})'.format(t2-t1))

    round2num = lambda x, num: num * (x // num)
    num_data_videos = round2num(len(data_video_paths), args.prefetch_batch_size)
    num_validation_videos = round2num(len(validation_video_paths), args.prefetch_batch_size)
    num_batch_videos = round2num(args.prefetch_batch_size, args.train_batch_size)
    assert(num_batch_videos == args.prefetch_batch_size) # make `prefetch_batch_size` a multiple of `train_batch_size`

    ############################################################################
    # begin the training process
    ############################################################################
    logging.info('begin training')
    validation_count = 0
    train_batch_count = 0
    train_minibatch_count = 0

    if os.path.isfile(args.train_progress_csv):
        with open(args.train_progress_csv, 'r') as f:
            train_batch_count = int(f.read().strip().split('\n')[-1].split(',')[0])
            train_batch_count += 1
            
    with open(args.train_progress_csv, 'a') as logfile:
        if not args.continue_training:
            if args.overfit or args.overfitt:
                logfile.write('# OVERFITTING! OVERFITTING! OVERFITTING! \n')
            logfile.write('# {}\n'.format(' '.join(sys.argv)))
            logfile.write('# {}\n'.format(args.model_name))
            logfile.write('# {}\n'.format(args.checkpoint_dir))
            logfile.write('# {}\n'.format(datetime.datetime.now()))
            logfile.write('# {}\n'.format(args))
            logfile.write('# model_name:{} data_size:{} validation_size:{} prefetch_batch_size:{} train_batch_size:{} num_labels:{} initial_learning_rate:{} learning_rate_decay:{} minimum_learning_rate:{} overfit:{}\n'.format(args.model_name, num_data_videos, num_validation_videos, args.prefetch_batch_size, args.train_batch_size, len(labels_num2str), args.learning_rate_init, args.learning_rate_decay, args.learning_rate_min, args.overfit or args.overfitt))
            logfile.write('# video_width:{} video_height:{} video_length:{} video_downsample_ratio:{} \n'.format(args.video_width, args.video_height, args.video_length, args.video_downsample_ratio))
            logfile.write('batch_num, epoch, global_step, learning_rate, train_acc_top1, train_acc_top3, train_acc_top5, train_loss, valid_acc_top1, valid_acc_top3, valid_acc_top5, valid_loss, checkpoint_path, model_export_path\n')
            logfile.flush()

        with mp.Pool(processes=mp.cpu_count(), initializer=pool_init, initargs=(shared_mem,)) as pool:
            
            if args.continue_training:                    
                saver.restore(sess, tf.train.latest_checkpoint(args.checkpoint_dir))
                if args.global_step_init is not None:
                    sess.run([learning_rate], feed_dict={step : args.global_step_init})
                    
            else:
                sess.run(tf.global_variables_initializer()) # initialize all variables
                model.initialize_variables() # set any model specific variables
                
            for epoch in range(args.num_epoch):
                video_paths = data_video_paths.copy()
                random.shuffle(video_paths)

                ############################################################
                # loop over all the videos in a random order
                ############################################################
                for batch_base_idx in range(0, num_data_videos, args.prefetch_batch_size):
                    video_paths_batch = video_paths[batch_base_idx:batch_base_idx+args.prefetch_batch_size]

                    logging.debug('begin load videos')
                    t1 = timeit.default_timer() 
                    video_batch = load_videos(pool, args.data_root, data_labels, video_paths_batch, args.video_width, args.video_height, args.video_length, args.video_downsample_ratio, True, args.prefetch_batch_size, shared_mem)
                    t2 = timeit.default_timer()
                    logging.debug('done! (load videos): {})'.format(t2-t1))

                    # print('out a video to make sure things are working')
                    # print(video_batch['video_labelnums'][0], labels_num2str[video_batch['video_labelnums'][0]])
                    # print(np.min(video_batch['video_rawframes'][0,0,:,:]), np.max(video_batch['video_rawframes'][0,0,:,:]))
                    # print(video_batch['video_rawframes'][0,0,:,:].dtype)
                    # for i in range(args.video_length):
                    #     frame = (np.clip(video_batch['video_rawframes'][0,i,:,:] + 116, 0, 255)).astype(dtype=np.uint8)
                    #     im = Image.fromarray(frame)
                    #     im.save('/home/jemmons/frame{}.jpg'.format(i))
                    # return 

                    assert(not np.any(np.isnan(video_batch['video_rawframes'])))

                    ########################################################
                    # train model
                    ########################################################
                    logging.debug('begin train batch')
                    t1 = timeit.default_timer()

                    cum_data_loss = 0
                    cum_data_correct_predictions = 0
                    cum_data_correct_predictions3 = 0
                    cum_data_correct_predictions5 = 0

                    for i in range(0, num_batch_videos, args.train_batch_size):
                        begin = i
                        end = i + args.train_batch_size
                        batch_labels = keras.utils.to_categorical(video_batch['video_labelnums'][begin:end], len(labels_num2str))
                        
                        _, _step, _learn_rate, _total_loss_val, _loss_vals, _predict_vals = sess.run([train_step, step, learning_rate, total_loss, losses, predictions],
                                                                                                     feed_dict={x_input : video_batch['video_rawframes'][begin:end,:,:,:],
                                                                                                                x_length : video_batch['video_numframes'][begin:end],
                                                                                                                y_label : batch_labels,
                                                                                                                y_zeros : zeros})
                        predict_analysis = []
                        for video_idx in range(args.train_batch_size):
                            video_labelnum = video_batch['video_labelnums'][begin:end][video_idx]
                            video_labelstr = labels_num2str[video_labelnum]
                            video_length = video_batch['video_numframes'][begin:end][video_idx]

                            fvec = []
                            fmax = []
                            fmax5 = []
                            fsum = []
                            floss = []
                            for frame_idx in range(video_length):
                                predict = _predict_vals[video_idx, frame_idx, :]
                                fvec.append(predict)
                                fsum.append(sum(predict))
                                fmax.append(np.argmax(predict))

                                predict_copy = predict.copy()
                                fmax5.append(predict_copy.argsort()[-5:][::-1])
                                
                                floss.append(_loss_vals[video_idx, :])

                            predict_analysis.append({
                                'frame_labelvec' : fvec,
                                'frame_labelmax' : fmax,
                                'frame_labelmax5' : fmax5,
                                'frame_probsum' : fsum,
                                'frame_loss' : floss,
                                'video_length' : video_length,
                                'video_labelnum' : video_labelnum,
                                'video_labelstr' : video_labelstr
                            })

                        for analysis in predict_analysis:
                            for a in analysis['frame_probsum']:
                                assert( abs(a - 1) < 0.00001 )

                            fmax_str = list(map(lambda x: str(x).zfill(2), analysis['frame_labelmax']))
                            fmax5_str = list(map(lambda x: str(x).zfill(2), analysis['frame_labelmax5'][-1]))
                            tl = str(analysis['video_labelnum']).zfill(2)
                            logging.debug('{} true_label,prediction: {},{} ({}) {}'.format('T' if tl==fmax_str[-1] else 'F', tl, fmax_str[-1], fmax5_str, fmax_str))
                            #logging.debug('{}'.format(analysis['frame_loss']))

                        assert(not np.isnan(_total_loss_val))
                        assert(not np.any(np.isnan(_predict_vals)))

                        predicted_vals = np.asarray([ p['frame_labelmax'][-1] for p in predict_analysis ])
                        correct_predictions = sum(video_batch['video_labelnums'][begin:end] == predicted_vals)

                        predicted_vals3 = [ l in p['frame_labelmax5'][-1][:3] for l,p in zip(video_batch['video_labelnums'][begin:end], predict_analysis) ]
                        correct_predictions3 = sum(predicted_vals3)

                        predicted_vals5 = [ l in p['frame_labelmax5'][-1] for l,p in zip(video_batch['video_labelnums'][begin:end], predict_analysis) ]
                        correct_predictions5 = sum(predicted_vals5)

                        cum_data_correct_predictions += correct_predictions
                        cum_data_correct_predictions3 += correct_predictions3
                        cum_data_correct_predictions5 += correct_predictions5
                        cum_data_loss += _total_loss_val

                        logging.info('TRAINING BATCH\taccuracy (top-1): {}'.format(correct_predictions / args.train_batch_size))
                        logging.info('TRAINING BATCH\taccuracy (top-3): {}'.format(correct_predictions3 / args.train_batch_size))
                        logging.info('TRAINING BATCH\taccuracy (top-5): {}'.format(correct_predictions5 / args.train_batch_size))
                        logging.info('TRAINING BATCH\tloss: {}'.format(_total_loss_val))
                        logging.info('TRAINING BATCH\tlearning_rate: {}'.format(_learn_rate))
                        logging.info('TRAINING BATCH\tnum_examples: {}'.format(args.train_batch_size))
                        train_minibatch_count += 1

                    logfile.write('{}, {}, {}, {}, {}, {}, {}, {}, , , , , ,\n'.format(train_batch_count, epoch, _step, _learn_rate, cum_data_correct_predictions/args.prefetch_batch_size, cum_data_correct_predictions3/args.prefetch_batch_size, cum_data_correct_predictions5/args.prefetch_batch_size, cum_data_loss))
                    logfile.flush()

                    t2 = timeit.default_timer()
                    logging.debug('done! (train batch: {})'.format(t2-t1))

                    ########################################################
                    # save the model and compute validation accuracy/loss
                    ########################################################
                    if validation_count == args.validation_interval:
                        validation_count = 0

                        ####################################################
                        # save the model
                        ####################################################
                        logging.info('SAVING MODEL to disk')
                        t1 = timeit.default_timer() 
                        checkpoint_path = os.path.join(args.checkpoint_dir, 'tf_checkpoint-epoch{}-batch-minibatch{}'.format(epoch, train_minibatch_count-1))
                        saver.save(sess, checkpoint_path)

                        model_weights_path = os.path.join(args.checkpoint_dir, 'model_weights-epoch{}-step{}.npz'.format(epoch, train_minibatch_count-1))
                        model.export_variables(model_weights_path)

                        t2 = timeit.default_timer()
                        logging.info('SAVING MODEL done! (save model to disk): {})'.format(t2 - t1))
                        saver.restore(sess, checkpoint_path)

                        ####################################################
                        # compute validation accuracy/loss
                        ####################################################
                        logging.debug('begin validation accuracy computation')
                        t1 = timeit.default_timer() 

                        cum_validation_loss = 0
                        cum_validation_correct_predictions = 0
                        cum_validation_correct_predictions3 = 0
                        cum_validation_correct_predictions5 = 0

                        for valid_base_idx in range(0, num_validation_videos, args.prefetch_batch_size):
                            validation_batch = load_videos(pool, args.data_root, validation_labels, validation_video_paths[valid_base_idx:valid_base_idx+args.prefetch_batch_size], args.video_width, args.video_height, args.video_length, args.video_downsample_ratio, False, args.prefetch_batch_size, shared_mem)
                        
                            # print('out a video to make sure things are working')
                            # print(validation_batch['video_labelnums'][0], labels_num2str[validation_batch['video_labelnums'][0]])
                            # print(np.min(validation_batch['video_rawframes'][0,0,:,:]), np.max(validation_batch['video_rawframes'][0,0,:,:]))
                            # print(validation_batch['video_rawframes'][0,0,:,:].dtype)
                            # for i in range(args.video_length):
                            #     frame = (np.clip(validation_batch['video_rawframes'][0,i,:,:] + 116, 0, 255)).astype(dtype=np.uint8)
                            #     im = Image.fromarray(frame)
                            #     im.save('/home/jemmons/frame{}.jpg'.format(i))
                            # return 

                            for i in range(0, num_batch_videos, args.train_batch_size):
                                begin = i
                                end = i + args.train_batch_size

                                _step, learn_rate, total_loss_val, predict_vals = sess.run([step, learning_rate, total_loss, predictions],
                                                                                           feed_dict={x_input : validation_batch['video_rawframes'][begin:end,:,:,:],
                                                                                                      x_length : validation_batch['video_numframes'][begin:end],
                                                                                                      y_label : keras.utils.to_categorical(validation_batch['video_labelnums'][begin:end],len(labels_num2str)),
                                                                                                      y_zeros : zeros})

                                assert(not np.isnan(total_loss_val))
                                assert(not np.any(np.isnan(predict_vals)))
                                
                                predict_analysis = []
                                for video_idx in range(args.train_batch_size):
                                    video_labelnum = validation_batch['video_labelnums'][begin:end][video_idx]
                                    video_labelstr = labels_num2str[video_labelnum]
                                    video_length = validation_batch['video_numframes'][begin:end][video_idx]

                                    fvec = []
                                    fmax = []
                                    fmax5 = []
                                    fsum = []
                                    floss = []
                                    for frame_idx in range(video_length):
                                        predict = predict_vals[video_idx, frame_idx, :]
                                        fvec.append(predict)
                                        fsum.append(sum(predict))
                                        fmax.append(np.argmax(predict))

                                        predict_copy = predict.copy()
                                        fmax5.append(predict_copy.argsort()[-5:][::-1])

                                    predict_analysis.append({
                                        'frame_labelvec' : fvec,
                                        'frame_labelmax' : fmax,
                                        'frame_labelmax5' : fmax5,
                                        'frame_probsum' : fsum,
                                        'video_length' : video_length,
                                        'video_labelnum' : video_labelnum,
                                        'video_labelstr' : video_labelstr
                                    })

                                for analysis in predict_analysis:
                                    for a in analysis['frame_probsum']:
                                        assert( abs(a - 1) < 0.00001 )

                                    fmax_str = list(map(lambda x: str(x).zfill(2), analysis['frame_labelmax']))
                                    fmax5_str = list(map(lambda x: str(x).zfill(2), analysis['frame_labelmax5'][-1]))
                                    tl = str(analysis['video_labelnum']).zfill(2)
                                    logging.debug('{} true_label,prediction: {},{} ({}) {}'.format('T' if tl==fmax_str[-1] else 'F', tl, fmax_str[-1], fmax5_str, fmax_str))

                                predicted_vals = np.asarray([ p['frame_labelmax'][-1] for p in predict_analysis ])
                                correct_predictions = sum(validation_batch['video_labelnums'][begin:end] == predicted_vals)

                                predicted_vals3 = [ l in p['frame_labelmax5'][-1][:3] for l,p in zip(validation_batch['video_labelnums'][begin:end], predict_analysis) ]
                                correct_predictions3 = sum(predicted_vals3)

                                predicted_vals5 = [ l in p['frame_labelmax5'][-1] for l,p in zip(validation_batch['video_labelnums'][begin:end], predict_analysis) ]
                                correct_predictions5 = sum(predicted_vals5)

                                cum_validation_correct_predictions += correct_predictions
                                cum_validation_correct_predictions3 += correct_predictions3
                                cum_validation_correct_predictions5 += correct_predictions5
                                cum_validation_loss += total_loss_val

                                logging.debug('VALIDATION batch accuracy (top-1): {}'.format(correct_predictions / args.train_batch_size))
                                logging.debug('VALIDATION batch accuracy (top-3): {}'.format(correct_predictions3 / args.train_batch_size))
                                logging.debug('VALIDATION batch accuracy (top-5): {}'.format(correct_predictions5 / args.train_batch_size))
                                logging.debug('VALIDATION batch loss: {}'.format(total_loss_val))

                        logfile.write('{}, {}, {}, {}, , , , , {}, {}, {}, {}, {}, {}\n'.format(train_batch_count, epoch, _step, learn_rate, cum_validation_correct_predictions/num_validation_videos, cum_validation_correct_predictions3/num_validation_videos, cum_validation_correct_predictions5/num_validation_videos, cum_validation_loss, checkpoint_path, model_weights_path))
                        logfile.flush()

                        logging.info('VALIDATION TOTAL\taccuracy (top-1): {}'.format(cum_validation_correct_predictions / num_validation_videos))
                        logging.info('VALIDATION TOTAL\taccuracy (top-3): {}'.format(cum_validation_correct_predictions3 / num_validation_videos))
                        logging.info('VALIDATION TOTAL\taccuracy (top-5): {}'.format(cum_validation_correct_predictions5 / num_validation_videos))
                        logging.info('VALIDATION TOTAL\tloss: {}'.format(cum_validation_loss))
                        logging.info('VALIDATION TOTAL\tnum_examples: {}'.format(num_validation_videos))

                        t2 = timeit.default_timer()
                        logging.debug('done! (validation accuracy computation): {})'.format(t2 - t1))

                        ####################################################
                        # restore model to the state prior to computing validation accuracy/loss
                        ####################################################
                        saver.restore(sess, checkpoint_path)

                    validation_count += 1
                    train_batch_count += 1

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='train the feedbacknet for the UCF-101 data set')

    # dataset parameters (all required)
    parser.add_argument('label_file', type=str, nargs=None,
                        help='labels file')

    parser.add_argument('validation_file', type=str, nargs=None,
                        help='validation label file')

    parser.add_argument('data_file', type=str, nargs=None,
                        help='data label file')

    parser.add_argument('data_root', type=str, nargs=None,
                        help='root of dataset')

    parser.add_argument('model_name', type=str, nargs=None,
                        help='name of the model to train')

    parser.add_argument('checkpoint_dir', type=str, nargs=None,
                        help='location to store checkpoints during training')

    parser.add_argument('train_progress_csv', type=str, nargs=None,
                        help='csv file to record the training and validation error/loss')

    parser.add_argument('--ucf101',
                        help='flag specifying the dataset being used',
                        action='store_true',
                        default=False)

    parser.add_argument('--twentybn',
                        help='flag specifying the dataset being used',
                        action='store_true',
                        default=False)
    
    # tuning parameters (optional)
    parser.add_argument('--overfit',
                        help='overfit on a small chunk [64 data points] of data (official run: False)',
                        action='store_true',
                        default=False)

    parser.add_argument('--overfitt',
                        help='overfit on a larger chunk of data (official run: False)',
                        action='store_true',
                        default=False)

    parser.add_argument('--fine_tune_vgg',
                        help='fine tune the pretrained vgg16 network',
                        action='store_true',
                        default=False)

    parser.add_argument('--continue_training',
                        help='load the most recent set of weights and continue training (official run: False)',
                        action='store_true',
                        default=False)

    parser.add_argument('--video_width', type=int, nargs=None,
                        help='the width to rescale all videos (official run: 176)',
                        default=112)

    parser.add_argument('--video_height', type=int, nargs=None,
                        help='the height to rescale all videos (official run: XXXX)',
                        default=112)

    parser.add_argument('--video_length', type=int, nargs=None,
                        help='the num frames to truncate all videos (official run: XXXX)',
                        default=20)
    
    parser.add_argument('--video_downsample_ratio', type=int, nargs=None,
                        help='the temporal downsampling ratio to use all the videos (official run: XXXX)',
                        default=5)
    
    parser.add_argument('--num_epoch', type=int, nargs=None,
                        help='number of training epochs over the whole dataset (official run: XXXX)',
                        default=1024)

    parser.add_argument('--last_loss_multipler', type=float, nargs=None,
                        help='the multipler to apply to the loss on the last frame of the video (official run: XXXX)',
                        default=10)

    parser.add_argument('--learning_rate_init', type=float, nargs=None,
                        help='initial vlaue for learning rate for the optimization (official run: XXXX)',
                        default=1e-1)

    parser.add_argument('--learning_rate_decay', type=float, nargs=None,
                        help='learning rate decay (per prefetch_batch) for the optimization (official run: XXXX)',
                        default=0.995)

    parser.add_argument('--learning_rate_min', type=float, nargs=None,
                        help='minimum learning rate for the optimization (official run: XXXX)',
                        default=1e-3)

    parser.add_argument('--global_step_init', type=float, nargs=None,
                        help='the initial global training step count (official run: XXXX)',
                        default=None)
    
    parser.add_argument('--train_batch_size', type=int, nargs=None,
                        help='training batch size; num videos per batch (official run: XXXX)',
                        default=16)

    parser.add_argument('--prefetch_batch_size', type=int, nargs=None,
                        help='number of videos to prefetch (official run: XXXX)',
                        default=1024)

    parser.add_argument('--validation_interval', type=int, nargs=None,
                        help='number of training batches to process before printing validation error (official run: XXXX)',
                        default=32)

    
    args = parser.parse_args()
    main(args)

