#!/usr/bin/env python3

import argparse
import logging
import csv
import os
import sys
import timeit
import random
import multiprocessing as mp
import numpy as np
import keras
import tensorflow as tf
import PIL
from PIL import Image

import vfeedbacknet as v

logging.basicConfig(level=logging.DEBUG)

def pool_init(mem_pool_):
    global mem_pool
    mem_pool = mem_pool_
            
def prepare_video(args):
    data_root, video_num, video_width, video_height, video_length, video_index, batch_size, pool_index = args

    video_mem = np.frombuffer(mem_pool[pool_index], np.ctypeslib.ctypes.c_float)
    video_mem = video_mem.reshape((batch_size, video_length, video_height, video_width, 3))

    pathgen = lambda x : os.path.join(data_root, str(video_num), x)
    frames = sorted( os.listdir(pathgen('')) )

    num_frames = min(len(frames), video_length)

    video_mem[video_index,:,:,:,:] = 0 # make grey
    for i in range(num_frames):
        image = Image.open(pathgen(frames[i])) # in RGB order by default
        image = np.asarray(image.resize((video_width, video_height), PIL.Image.BICUBIC), dtype=np.float32)
        video_mem[video_index,i,:,:,:] = (image / 128) - 1 # squash to interval (-1, 1)
        
    return { 'num_frames' : num_frames, 'video_num' : video_num }

def load_videos(pool, data_root, data_labels, video_nums, video_width, video_height, video_length, batch_size, mem_pool, pool_index):

    logging.debug('begin loading videos')
    prepare_video_jobs = [ (data_root, video_nums[i], video_width, video_height, video_length, i, batch_size, pool_index) for i in range(batch_size) ]
    prepared_videos = pool.map(prepare_video, prepare_video_jobs)
    logging.debug('done! (loading videos)')

    video_numframes = np.zeros((batch_size,), dtype=np.int32)
    video_labelnums = np.zeros((batch_size,), dtype=np.int32)

    for i in range(batch_size):
        video_numframes[i] = prepared_videos[i]['num_frames']
        video_labelnums[i] = data_labels[ prepared_videos[i]['video_num'] ]
        
    video_mem = np.frombuffer(mem_pool[pool_index], np.ctypeslib.ctypes.c_float)
    video_mem = video_mem.reshape((batch_size, video_length, video_height, video_width, 3))

    batch = {
        'num_videos' : batch_size,
        'video_rawframes' : video_mem,
        'video_numframes' : video_numframes,
        'video_labelnums' : video_labelnums,
    }

    return batch

def main(args):
    with open(args.label_file, 'r') as csvfile:
        reader = csv.reader(csvfile, delimiter=';')
        labels_num2str = [ item[0].lower() for item in reader ]
        labels_str2num =  { label : idx  for idx,label in zip(range(len(labels_num2str)), labels_num2str) }

    with open(args.validation_file, 'r') as csvfile:
        reader = csv.reader(csvfile, delimiter=';')
        validation_labels = { int(item[0]) : labels_str2num[item[1].lower()] for item in reader }; 
        validation_video_nums = np.asarray( list(validation_labels.keys()) )
        
    with open(args.data_file, 'r') as csvfile:
        reader = csv.reader(csvfile, delimiter=';')
        data_labels = { int(item[0]) : labels_str2num[item[1].lower()] for item in reader }; 
        data_video_nums = np.asarray( list(data_labels.keys()) )

    # build model
    x_input = tf.placeholder(tf.float32, [None, args.video_length, args.video_height, args.video_width, 3], name='input')
    x_length = tf.placeholder(tf.int32, [None,], name='input_length')
    y_label = tf.placeholder(tf.float32, [None, len(labels_num2str)], name='label')
    y_zeros = tf.placeholder(tf.float32, [None,],  name='zeros')

    # loss, correct_prediction, accruacy = v.vfeedback_model_nofeedback(args, x_input, y_label, x_length, y_zeros)
    # train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)

    # allocate memory up front
    logging.debug('begin allocate memory buffers')
    t1 = timeit.default_timer()
    mem_pool = [mp.RawArray(np.ctypeslib.ctypes.c_float, args.prefetch_batch_size*args.video_length*args.video_height*args.video_width*3) for _ in range(args.mem_pool_size)]
    t2 = timeit.default_timer()
    logging.debug('done! (allocate memory buffers: {})'.format(t2-t1))
    
    round2batch = lambda x: args.prefetch_batch_size * (x // args.prefetch_batch_size)
    num_data_videos = round2batch(len(data_video_nums))

    # get batch of data and train
    with mp.Pool(processes=mp.cpu_count(), initializer=pool_init, initargs=(mem_pool,)) as pool:
        mem_pool_index = 0
        
        with tf.Session() as sess:
            sess.run(tf.global_variables_initializer())
            
            for epoch in range(args.num_epoch):
                video_nums = data_video_nums.copy()
                random.shuffle(video_nums)
                
                for i in range(0, num_data_videos, args.prefetch_batch_size):
                    video_nums_batch = video_nums[i:i+args.prefetch_batch_size]

                    t1 = timeit.default_timer() 
                    video_batch = load_videos(pool, args.data_root, data_labels, video_nums_batch, args.video_width, args.video_height, args.video_length, args.prefetch_batch_size, mem_pool, mem_pool_index)
                    mem_pool_index = (mem_pool_index + 1) % args.mem_pool_size
                    t2 = timeit.default_timer()
                    print('batch creation time:', t2 - t1)

                    # for i in range(40):
                    #     frame = (128*(video_batch['video_rawframes'][0,i,:,:,:] + 1)).astype(dtype=np.uint8)
                    #     im = Image.fromarray(frame)
                    #     im.save('/home/jemmons/frame{}.jpg'.format(i))
                        

                    # train the model
                    # for i in range(0, batch['num_videos']-TRAINING_BATCH_SIZE+1, TRAINING_BATCH_SIZE):
                    #     begin = i
                    #     end = i + TRAINING_BATCH_SIZE
                    #     print('starting a training batch')
                    
                    #     train_step.run(feed_dict={ 
                    #         x_input : batch['video_rawframes'][begin:end,:,:,:,:],
                    #         x_length : batch['video_numframes'][begin:end],
                    #         y_label : keras.utils.to_categorical(batch['video_labelnums'][begin:end], len(args.labels)),
                    #         y_zeros : np.zeros((TRAINING_BATCH_SIZE,))})

                    #     print('completed an iteration!')

                
                #t1 = timeit.default_timer() 
                #validation_set = load_videos(pool, mem_pool, args.data_root, validation_video_nums, args.video_width, args.video_height, args.video_length, len(validation_video_nums))
                #t2 = timeit.default_timer()
                #print('done loading validation set!', t2 - t1)
                
    # save the trained model!
    # ...

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='train the feedbacknet for the 20BN data set')

    # dataset parameters
    parser.add_argument('label_file', type=str, nargs=None,
                        help='20bn labels file (e.g. jester-v1-labels.csv)')

    parser.add_argument('validation_file', type=str, nargs=None,
                        help='20bn validation label file (e.g. jester-v1-validation.csv)')

    parser.add_argument('data_file', type=str, nargs=None,
                        help='20bn data label file (e.g. jester-v1-train.csv)')

    parser.add_argument('data_root', type=str, nargs=None,
                        help='root of 20bn dataset (e.g. ~/20bn-datasets/20bn-jester-v1)')

    # tuning parameters
    parser.add_argument('--video_width', type=int, nargs=None,
                        help='the width to rescale all videos (official run: 176 for 20bn)',
                        default=176)

    parser.add_argument('--video_height', type=int, nargs=None,
                        help='the height to rescale all videos (official run: 100 for 20bn)',
                        default=100)

    parser.add_argument('--video_length', type=int, nargs=None,
                        help='the num frames to truncate all videos (official run: 40 for 20bn)',
                        default=40)

    parser.add_argument('--num_epoch', type=int, nargs=None,
                        help='number of training epochs over the whole dataset (official run: 16 for 20bn)',
                        default=1)

    parser.add_argument('--train_batch_size', type=int, nargs=None,
                        help='training batch size; num videos per batch (official run: XXX for 20bn)',
                        default=1)

    parser.add_argument('--prefetch_batch_size', type=int, nargs=None,
                        help='number of videos to prefetch (official run: 1024 for 20bn)',
                        default=1)

    parser.add_argument('--mem_pool_size', type=int, nargs=None,
                        help='number of batches to preallocate in memory pool (official run: 1 for 20bn)',
                        default=1)
    
    args = parser.parse_args()
    main(args)

