#!/usr/bin/env python3

import asyncio
import argparse
import datetime
import logging
import csv
import os
import sys
import socket
import timeit
import random

import numpy as np
import scipy.stats as stats
import multiprocessing as mp
import sharedmem as sm

import keras
import tensorflow as tf
from tensorflow.python.client import device_lib

import vfeedbacknet as v
from vfeedbacknet import TrainingLogger, ModelLogger
from vfeedbacknet import pool_init, prepare_video, load_videos

from PIL import Image

#logging.basicConfig(level=logging.INFO)
logging.basicConfig(level=logging.DEBUG)

def main(args):
    ############################################################################
    # open and parse the data and label files
    ############################################################################
    logging.info('loading input files')
    assert args.ucf101 ^ args.jester ^ args.something_something ^ args.imagenet, 'you must specify exactly one dataset to use for training (XOR)'

    with open(args.label_file, 'r') as csvfile:
        labels_num2str = None
        labels_str2num = None
        
        if args.ucf101:
            reader = csv.reader(csvfile, delimiter=' ')
            labels_num2str = [ item[1] for item in reader ]
            labels_str2num =  { label : idx  for idx,label in zip(range(len(labels_num2str)), labels_num2str) }
        elif args.jester or args.something_something:
            reader = csv.reader(csvfile, delimiter=';')
            labels_num2str = [ item[0].lower() for item in reader ]
            labels_str2num =  { label : idx  for idx,label in zip(range(len(labels_num2str)), labels_num2str) }
        elif args.imagenet:
            reader = csv.reader(csvfile, delimiter=' ')
            labels_num2str = [ item[0].lower() for item in reader ]
            labels_str2num =  { label : idx  for idx,label in zip(range(len(labels_num2str)), labels_num2str) }            
            
    with open(args.validation_file, 'r') as csvfile:
        validation_video_paths = None
        validation_labels = None

        if args.ucf101:
            reader = csv.reader(csvfile, delimiter=' ')
            validation_video_paths = [ item[0].split('.')[0] for item in reader ] 
            validation_labels = { item : labels_str2num[item.split('/')[0]] for item in validation_video_paths } 
        elif args.jester or args.something_something:
            reader = csv.reader(csvfile, delimiter=';')
            validation_labels = {}
            for item in reader:
                basename = str(int(item[0]))
                dirname = basename[-1]+'.dir'
                video_path = os.path.join(dirname, basename)
                validation_labels[video_path] = labels_str2num[item[1].lower()]
            validation_video_paths = list(validation_labels.keys())
        elif args.imagenet:
            reader = csv.reader(csvfile, delimiter=' ')
            validation_labels = { item[0] : labels_str2num[item[0].split('/')[1].lower()] for item in reader }
            validation_video_paths = list(validation_labels.keys())
            
        random.seed(123) # shuffle with this particular seed to help with debugging
        random.shuffle(validation_video_paths)

        if args.overfit:
            logging.info('OVERFITTING!')
            args.prefetch_batch_size = 16
            args.train_batch_size = 16
            validation_video_paths = validation_video_paths[:args.prefetch_batch_size] # force overfitting
        elif args.overfitt:
            logging.info('OVERFITTING!')
            args.prefetch_batch_size = 1024
            validation_video_paths = validation_video_paths[:args.prefetch_batch_size] # force overfitting
            
    with open(args.data_file, 'r') as csvfile:
        data_video_paths = None
        data_labels = None

        if args.ucf101:
            reader = csv.reader(csvfile, delimiter=' ')
            data_video_paths = [ item[0].split('.')[0] for item in reader ] 
            data_labels = { item : labels_str2num[item.split('/')[0]] for item in data_video_paths } 
        elif args.jester or args.something_something:
            reader = csv.reader(csvfile, delimiter=';')
            data_labels = {}
            for item in reader:
                basename = str(int(item[0]))
                dirname = basename[-1]+'.dir'
                video_path = os.path.join(dirname, basename)
                data_labels[video_path] = labels_str2num[item[1].lower()]
            data_video_paths = list(data_labels.keys())
        elif args.imagenet:
            reader = csv.reader(csvfile, delimiter=' ')
            data_labels = { item[0] : labels_str2num[item[0].split('/')[1].lower()] for item in reader } 
            data_video_paths = list(data_labels.keys())

        random.seed(123) # shuffle with this particular seed to help with debugging
        random.shuffle(data_video_paths)

        if args.overfit:
            logging.info('OVERFITTING!')
            data_video_paths = data_video_paths[:args.prefetch_batch_size] # force overfitting
        elif args.overfitt:
            logging.info('OVERFITTING!')
            data_video_paths = data_video_paths[:args.prefetch_batch_size] # force overfitting

    # local_device_protos = device_lib.list_local_devices()
    # available_gpus = [x.name for x in local_device_protos if x.device_type == 'GPU']
    # num_gpus = len(available_gpus)
    # logging.debug('there are {} available gpus: {}'.format(num_gpus, available_gpus))

    ############################################################################
    # build model
    ############################################################################
    logging.info('building model: {}'.format(args.model_name))
    config = tf.ConfigProto()
    config.allow_soft_placement = True
    #config.log_device_placement = True # will generate A LOT of log information
    if socket.gethostname() == 'robocop' or socket.gethostname()[:2] == 'xs':
        logging.debug('We are on robocop (or xstream)! we can be less afraid that mean people will consume GPU memory')
        config.gpu_options.allow_growth = True
    #config.gpu_options.per_process_gpu_memory_fraction = .98
    sess = tf.Session(config=config)

    # x_input = tf.placeholder(tf.float32,
    #                          [args.train_batch_size, args.video_length,
    #                           args.video_height, args.video_width],
    #                          name='x_input')
    # x_length = tf.placeholder(tf.int32, [args.train_batch_size,], name='x_length')
    
    # logging.debug('creating model with {} outputs'.format(len(labels_num2str)))
    # y_label = tf.placeholder(tf.float32, [args.train_batch_size, len(labels_num2str)], name='y_label')
    
    # y_zeros = tf.placeholder(tf.float32, [args.train_batch_size,],  name='y_zeros')
    # zeros = np.zeros((args.train_batch_size,))

    x_input = tf.placeholder(tf.float32,
                             [None, args.video_length,
                              args.video_height, args.video_width, 3],
                             name='x_input')
    x_length = tf.placeholder(tf.int32, [None,], name='x_length')
    
    logging.debug('creating model with {} outputs'.format(len(labels_num2str)))
    y_label = tf.placeholder(tf.float32, [None, len(labels_num2str)], name='y_label')
    
    y_zeros = tf.placeholder(tf.float32, [None,],  name='y_zeros')
    zeros = np.zeros((args.train_batch_size,))

    assert args.num_gpus == 0 or args.train_batch_size % args.num_gpus == 0, 'train_batch_size must be divisible by num_gpus'

    model_generator_dict = {
        'vfeedbacknet_vgg' : lambda : v.vfeedbacknet_vgg.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                       train_featurizer='FROM_SCRATCH', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH'),
        'vfeedbacknet_fb_base' : lambda : v.vfeedbacknet_fb_base.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                       train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                       weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_fb_base1' : lambda : v.vfeedbacknet_fb_base1.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                         train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                         weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_fb_base2' : lambda : v.vfeedbacknet_fb_base2.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                         train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                         weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_model28' : lambda : v.vfeedbacknet_model28.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                       train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                       weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_model29' : lambda : v.vfeedbacknet_model29.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                       train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                       weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_model1' : lambda : v.vfeedbacknet_eccv_model1.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                               train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                               weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_model2' : lambda : v.vfeedbacknet_eccv_model2.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                               train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                               weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_model3' : lambda : v.vfeedbacknet_eccv_model3.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                               train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                               weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_model4' : lambda : v.vfeedbacknet_eccv_model4.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                               train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                               weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_model5' : lambda : v.vfeedbacknet_eccv_model5.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                               train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                               weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_model6' : lambda : v.vfeedbacknet_eccv_model6.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                               train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                               weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_model7' : lambda : v.vfeedbacknet_eccv_model7.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                               train_featurizer='FROM_SCRATCH', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                               weights_filename=None),
        'vfeedbacknet_eccv_model8' : lambda : v.vfeedbacknet_eccv_model8.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                               train_featurizer='FROM_SCRATCH', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                               weights_filename=None),
        'vfeedbacknet_eccv_model9' : lambda : v.vfeedbacknet_eccv_model9.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                               train_featurizer='FROM_SCRATCH', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                               weights_filename=None),
        'vfeedbacknet_eccv_model10' : lambda : v.vfeedbacknet_eccv_model10.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                               train_featurizer='FROM_SCRATCH', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                               weights_filename=None),
        'vfeedbacknet_eccv_model11' : lambda : v.vfeedbacknet_eccv_model11.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                                train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_model12' : lambda : v.vfeedbacknet_eccv_model12.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                                train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_model13' : lambda : v.vfeedbacknet_eccv_model13.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                                train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_model14' : lambda : v.vfeedbacknet_eccv_model14.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                                train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_model15' : lambda : v.vfeedbacknet_eccv_model15.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                                train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_model16' : lambda : v.vfeedbacknet_eccv_model16.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                                train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_model17' : lambda : v.vfeedbacknet_eccv_model17.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                                train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        #'vfeedbacknet_eccv_model18' : lambda : v.vfeedbacknet_eccv_model18.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
        #                                                                        train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
        #                                                                        weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        #'vfeedbacknet_eccv_model19' : lambda : v.vfeedbacknet_eccv_model19.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
        #                                                                        train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
        #                                                                        weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        #'vfeedbacknet_eccv_model20' : lambda : v.vfeedbacknet_eccv_model20.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
        #                                                                        train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
        #                                                                        weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_small_model1' : lambda : v.vfeedbacknet_eccv_small_model1.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                                    train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                    weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_small_model2' : lambda : v.vfeedbacknet_eccv_small_model2.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                                    train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                    weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_small_model4' : lambda : v.vfeedbacknet_eccv_small_model4.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                                    train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                    weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_medium_model1' : lambda : v.vfeedbacknet_eccv_medium_model1.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                                    train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                    weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_medium_model2' : lambda : v.vfeedbacknet_eccv_medium_model2.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                                    train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                    weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_medium_model4' : lambda : v.vfeedbacknet_eccv_medium_model4.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                                    train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                    weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_large_model1' : lambda : v.vfeedbacknet_eccv_large_model1.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                                    train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                    weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_large_model2' : lambda : v.vfeedbacknet_eccv_large_model2.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                                    train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                    weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_large_model4' : lambda : v.vfeedbacknet_eccv_large_model4.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                                    train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                    weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_videoLSTM_2' : lambda : v.vfeedbacknet_videoLSTM_2.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                                        train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                        weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_videoLSTM_2' : lambda : v.vfeedbacknet.vfeedbacknet_eccv_videoLSTM_2.Model(sess, len(labels_num2str), 
                                                                                        train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                        weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_videoLSTM_2_with_fb1' : lambda : v.vfeedbacknet.vfeedbacknet_eccv_videoLSTM_2_with_fb1.Model(sess, len(labels_num2str), 
                                                                                        train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                        weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_videoLSTM_2_with_fb2' : lambda : v.vfeedbacknet.vfeedbacknet_eccv_videoLSTM_2_with_fb2.Model(sess, len(labels_num2str), 
                                                                                        train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                        weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_bigfb_model1' : lambda : v.vfeedbacknet_eccv_bigfb_model1.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                                    train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                    weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_bigfb_model4' : lambda : v.vfeedbacknet_eccv_bigfb_model4.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                                    train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                    weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_simple_eccv_model1' : lambda : v.vfeedbacknet_simple_eccv_model1.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                                        train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                        weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_simple_eccv_model1_debug' : lambda : v.vfeedbacknet_simple_eccv_model1_debug.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                                                         train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                                         weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_bigfb_model1_debug' : lambda : v.vfeedbacknet_eccv_bigfb_model1_debug.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
	                                                                                    train_featurizer='NO', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH',
                                                                                            weights_filename=os.path.join(args.pretrain_root_prefix, 'vfeedbacknet_base_weights.npz')),
        'vfeedbacknet_eccv_base1' : lambda : v.vfeedbacknet_eccv_base1.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                           train_featurizer='FROM_SCRATCH', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH'),
        'vfeedbacknet_eccv_base2' : lambda : v.vfeedbacknet_eccv_base2.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                           train_featurizer='FROM_SCRATCH', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH'),
        'vfeedbacknet_eccv_base3' : lambda : v.vfeedbacknet_eccv_base3.Model(sess, len(labels_num2str), args.train_batch_size // args.num_gpus,
                                                                           train_featurizer='FROM_SCRATCH', train_main_model='FROM_SCRATCH', train_fc='FROM_SCRATCH'),
     }

    model = None
    with tf.device('/cpu:0'):
        model = model_generator_dict[args.model_name]()

        x_input_split = x_input
        x_length_split = x_length

        if args.num_gpus > 0:
            x_input_split = tf.split(x_input, num_or_size_splits=args.num_gpus, axis=0)
            x_length_split = tf.split(x_length, num_or_size_splits=args.num_gpus, axis=0)
        
    logits_ = []
    for gpu_num in range(args.num_gpus):
        with tf.device('/gpu:{}'.format(gpu_num)):
            logging.debug('-- placing ops on gpu:{} --'.format(gpu_num))

            tmp = model(x_input_split[gpu_num], x_length_split[gpu_num])
            logits_.append( tmp )

    assert len(logits_) == 0 or args.num_gpus > 0, 'this is likely a bug in the code...'

    # fallback if CPU training is requested
    if len(logits_) == 0:
        with tf.device('/cpu:0'):
            logits_ = [ model(x_input_split, x_length_split) ]
            
    with tf.device('/cpu:0'):
        logging.debug('-- placing ops on cpu:0 --')

        logits_ = tf.concat(logits_, axis=0)
        ModelLogger.log('combined_logits1', logits_)

        logits_ = tf.unstack(logits_, axis=1)
        ModelLogger.log('combined_logits2', logits_)

        logits_ = [ tf.unstack(l, axis=1) for l in logits_ ]
        logging.debug('{} feedback iterations detected!'.format(len(logits_)))
        
        # output is list of tuples: [(losses, total_loss, predictions), ...]
        # apply the extra loss penalty for the last element of the TEMPORAL iteration
        outputs = []
        for i in range(len(logits_)):
            multipler = args.last_loss_multipler if i == len(logits_)-1 else 1
            outputs += [ v.vfeedbacknet_lossfunctions.basic_loss_pred(logits_[i],
                                                                      x_length,
                                                                      len(logits_[i]), #args.video_length,
                                                                      y_label,
                                                                      y_zeros,
                                                                      last_loss_multiple=multipler) ]

        # apply the extra loss penalty for the last element of the FB SPATIAL iteration
        total_loss = 0
        for i in range(len(outputs)):
            output = outputs[i]
            multipler = args.last_loss_multipler if i == len(outputs)-1 else 1
            
            total_loss += multipler * output[1]
        
        losses = tf.stack(list(map(lambda x: x[0], outputs)), axis=1, name='losses_agg')
        predictions = tf.stack(list(map(lambda x: x[2], outputs)), axis=1, name='predictions_agg')
        ModelLogger.log('losses_agg', losses)
        ModelLogger.log('predictions_agg', predictions)

    logging.info('compute loss')
    t1 = timeit.default_timer()        
    step = tf.Variable(args.global_step_init if (args.global_step_init is not None) else 0, trainable=False, name='step')
    learning_rate = tf.maximum(tf.train.exponential_decay(args.learning_rate_init, step, args.prefetch_batch_size//args.train_batch_size, args.learning_rate_decay), args.learning_rate_min, name='learning_rate')
    optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate, name='optimizer')
    train_step = optimizer.minimize(total_loss, colocate_gradients_with_ops=True, global_step=step)
    t2 = timeit.default_timer()
    logging.debug('done! (compute loss: {})'.format(t2-t1))

    logging.info('creating saver')
    t1 = timeit.default_timer()
    saver = tf.train.Saver()
    t2 = timeit.default_timer()
    logging.debug('done! (create saver: {})'.format(t2-t1))

    ############################################################################
    # allocate shared memory up front 
    ############################################################################
    logging.info('allocating memory')
    logging.debug('begin allocate memory buffers')
    t1 = timeit.default_timer()
    shared_mem_idx = 0
    next_shared_mem_idx = 1
    shared_mem = [ sm.empty(args.prefetch_batch_size*args.video_length*args.video_height*args.video_width*3, dtype='f') for _ in range(2) ]
    t2 = timeit.default_timer()
    logging.debug('done! (allocate memory buffers: {})'.format(t2-t1))

    round2num = lambda x, num: num * (x // num)
    num_data_videos = round2num(len(data_video_paths), args.prefetch_batch_size)
    num_validation_videos = round2num(len(validation_video_paths), args.prefetch_batch_size)
    num_batch_videos = round2num(args.prefetch_batch_size, args.train_batch_size)
    assert num_batch_videos == args.prefetch_batch_size, 'num_batch_videos:{}, args.train_batch_size:{} args.prefetch_batch_size:{}'.format(num_batch_videos, args.train_batch_size, args.prefetch_batch_size) # make `prefetch_batch_size` a multiple of `train_batch_size`

    ############################################################################
    # begin the training process
    ############################################################################
    logging.info('begin training')
    validation_count = 0
    train_batch_count = 0
    train_minibatch_count = 0

    if os.path.isfile(args.train_progress_csv):
        with open(args.train_progress_csv, 'r') as f:
            train_batch_count = int(f.read().strip().split('\n')[-1].split(',')[0])
            train_batch_count += 1
            
    with open(args.train_progress_csv, 'a') as logfile:
        if not args.continue_training:
            if args.overfit or args.overfitt:
                logfile.write('# OVERFITTING! OVERFITTING! OVERFITTING! \n')
            logfile.write('# {}\n'.format(' '.join(sys.argv)))
            logfile.write('# {}\n'.format(args.model_name))
            logfile.write('# {}\n'.format(args.checkpoint_dir))
            logfile.write('# {}\n'.format(datetime.datetime.now()))
            logfile.write('# {}\n'.format(args))
            logfile.write('# model_name:{} data_size:{} validation_size:{} prefetch_batch_size:{} train_batch_size:{} num_labels:{} initial_learning_rate:{} learning_rate_decay:{} minimum_learning_rate:{} overfit:{}\n'.format(args.model_name, num_data_videos, num_validation_videos, args.prefetch_batch_size, args.train_batch_size, len(labels_num2str), args.learning_rate_init, args.learning_rate_decay, args.learning_rate_min, args.overfit or args.overfitt))
            logfile.write('# video_width:{} video_height:{} video_length:{} video_downsample_ratio:{} \n'.format(args.video_width, args.video_height, args.video_length, args.video_downsample_ratio))
            logfile.write('batch_num, epoch, global_step, learning_rate, feedback_iteration, train_acc_top1, train_acc_top3, train_acc_top5, train_loss, valid_acc_top1, valid_acc_top3, valid_acc_top5, valid_loss, checkpoint_path, model_export_path\n')
            #logfile.flush()

        logging.info('starting mutiprocessing pool')
        with mp.Pool(processes=args.num_cpus, initializer=pool_init, initargs=(shared_mem,)) as pool:
            logging.info('Done! starting mutiprocessing pool')
            
            if args.continue_training:                    
                logging.info('restoring variables')
                saver.restore(sess, tf.train.latest_checkpoint(args.checkpoint_dir))
                if args.global_step_init is not None:
                    sess.run([learning_rate], feed_dict={step : args.global_step_init}) 
                logging.info('Done! restoring variables')
                   
            else:
                logging.info('initializing variables')
                sess.run(tf.global_variables_initializer()) # initialize all variables
                model.initialize_variables() # set any model specific variables
                logging.info('Done! initializing variables')

                        
            for epoch in range(args.num_epoch):
                video_paths = data_video_paths.copy()
                random.shuffle(video_paths)

                logging.info('loading initial batch of videos')
                t1 = timeit.default_timer()
                video_batch_f = load_videos(pool, args.data_root, data_labels, video_paths[0:args.prefetch_batch_size], args.video_width, args.video_height, args.video_length, args.video_downsample_ratio, True, args.prefetch_batch_size, shared_mem, shared_mem_idx, is_ucf101=args.ucf101, is_imagenet=args.imagenet, is_zipped=args.imagenet_zipped)
                video_batch = video_batch_f() 
                t2 = timeit.default_timer()
                logging.info('Done! loading initial batch of videos: {}'.format(t2-t1))

                ############################################################
                # loop over all the videos in a random order
                ############################################################
                for batch_base_idx in range(0, num_data_videos, args.prefetch_batch_size):

                    logging.debug('model name: {}'.format(args.model_name))
                    logging.debug('begin load videos future')
                    t1 = timeit.default_timer() 
                    # prefetch the batch ahead of the index (therefore, we must skip loading on the last loop iteration to avoid array overrun)
                    if batch_base_idx+args.prefetch_batch_size < num_data_videos:
                        video_paths_batch = video_paths[batch_base_idx+args.prefetch_batch_size:batch_base_idx+2*args.prefetch_batch_size]
                        next_shared_mem_idx = (shared_mem_idx + 1) % 2
                        video_batch_f = load_videos(pool, args.data_root, data_labels, video_paths_batch, args.video_width, args.video_height, args.video_length, args.video_downsample_ratio, True, args.prefetch_batch_size, shared_mem, next_shared_mem_idx, is_ucf101=args.ucf101, is_imagenet=args.imagenet, is_zipped=args.imagenet_zipped)
                    t2 = timeit.default_timer()
                    logging.debug('done! (load videos future): {})'.format(t2-t1))
                    
                    # print('print out a video to make sure things are working')
                    # print(video_batch['video_labelnums'][0], labels_num2str[video_batch['video_labelnums'][0]])
                    # print(np.min(video_batch['video_rawframes'][0,0,:,:]), np.max(video_batch['video_rawframes'][0,0,:,:]))
                    # print(video_batch['video_rawframes'][0,0,:,:].dtype)
                    # for i in range(args.video_length):
                    #     frame = (np.clip(video_batch['video_rawframes'][0,i,:,:] + 116, 0, 255)).astype(dtype=np.uint8)
                    #     im = Image.fromarray(frame)
                    #     im.save('/home/jemmons/data/frame{}.jpg'.format(i))
                    # return 

                    assert(not np.any(np.isnan(video_batch['video_rawframes'])))

                    ########################################################
                    # train model
                    ########################################################
                    logging.debug('begin train batch')
                    t1_train = timeit.default_timer()

                    cum_data_loss = 0
                    cum_data_correct_predictions_l = []
                    cum_data_correct_predictions3_l = []
                    cum_data_correct_predictions5_l = []


                    for i in range(0, num_batch_videos, args.train_batch_size):
                        begin = i
                        end = i + args.train_batch_size
                        batch_labels = keras.utils.to_categorical(video_batch['video_labelnums'][begin:end], len(labels_num2str))
                        
                        t1_trainstep = timeit.default_timer()
                        logging.debug('begin forward/back propagation')
                        _, _step, _learn_rate, _total_loss_val, _loss_vals, _predict_vals = sess.run([train_step, step, learning_rate, total_loss, losses, predictions],
                                                                                                     feed_dict={x_input : video_batch['video_rawframes'][begin:end,:,:,:],
                                                                                                                x_length : video_batch['video_numframes'][begin:end],
                                                                                                                y_label : batch_labels,
                                                                                                                y_zeros : zeros})

                        logging.debug('learning_rate: {}'.format(_learn_rate))
                        assert(not np.isnan(_total_loss_val))
                        assert(not np.any(np.isnan(_predict_vals)))
                        t2_trainstep = timeit.default_timer()
                        logging.debug('Done! forward/back propagation: {}'.format(t2_trainstep-t1_trainstep))

                        t1_log = timeit.default_timer()
                        if train_batch_count % max((args.validation_interval//2),1) == 0:
                            analysis = TrainingLogger.process_prediction(_predict_vals, video_batch['video_labelnums'][begin:end], video_batch['video_numframes'][begin:end])

                            if len(cum_data_correct_predictions_l) == 0:
                                for feedback_idx in range(_predict_vals.shape[1]):
                                    cum_data_correct_predictions_l.append([])
                                    cum_data_correct_predictions3_l.append([])
                                    cum_data_correct_predictions5_l.append([])

                            for feedback_idx in range(_predict_vals.shape[1]):
                                correct_predictions = analysis[feedback_idx]['correct_predictions']
                                correct_predictions3 = analysis[feedback_idx]['correct_predictions3']
                                correct_predictions5 = analysis[feedback_idx]['correct_predictions5']

                                cum_data_correct_predictions_l[feedback_idx].append(correct_predictions)
                                cum_data_correct_predictions3_l[feedback_idx].append(correct_predictions3)
                                cum_data_correct_predictions5_l[feedback_idx].append(correct_predictions5)

                                logging.info('TRAINING BATCH\taccuracy (top-1) (feedback:{}): {}'.format(feedback_idx, correct_predictions / args.train_batch_size))
                                logging.info('TRAINING BATCH\taccuracy (top-3) (feedback:{}): {}'.format(feedback_idx, correct_predictions3 / args.train_batch_size))
                                logging.info('TRAINING BATCH\taccuracy (top-5) (feedback:{}): {}'.format(feedback_idx, correct_predictions5 / args.train_batch_size))

                            logging.info('TRAINING BATCH\tloss: {}'.format(_total_loss_val))
                            logging.info('TRAINING BATCH\tlearning_rate: {}'.format(_learn_rate))
                            logging.info('TRAINING BATCH\tnum_examples: {}'.format(args.train_batch_size))

                            cum_data_loss += _total_loss_val

                            train_minibatch_count += 1

                        t2_log = timeit.default_timer()
                        logging.debug('done! (train step: {})'.format(t2_trainstep-t1_trainstep))
                        logging.debug('done! (train log: {})'.format(t2_log-t1_log))
                        
                    if train_batch_count % args.validation_interval == 0:
                        for feedback_idx in range(len(cum_data_correct_predictions_l)):
                            cum_data_correct_predictions = sum(cum_data_correct_predictions_l[feedback_idx])
                            cum_data_correct_predictions3 = sum(cum_data_correct_predictions3_l[feedback_idx])
                            cum_data_correct_predictions5 = sum(cum_data_correct_predictions5_l[feedback_idx])

                            logfile.write('{}, {}, {}, {}, {}, {}, {}, {}, {}, , , , , ,\n'.format(train_batch_count, epoch, _step, _learn_rate, feedback_idx, cum_data_correct_predictions/args.prefetch_batch_size, cum_data_correct_predictions3/args.prefetch_batch_size, cum_data_correct_predictions5/args.prefetch_batch_size, cum_data_loss))
                        #logfile.flush()
                                        
                    logging.debug('begin load videos complete-future')
                    t1 = timeit.default_timer() 
                    video_batch = video_batch_f()
                    shared_mem_idx = next_shared_mem_idx
                    t2 = timeit.default_timer()
                    logging.debug('done! (load videos complete-future): {})'.format(t2-t1))

                    t2_train = timeit.default_timer()
                    logging.debug('done! (train batch: {})'.format(t2_train-t1_train))

                    logfile.flush()
                    ########################################################
                    # save the model and compute validation accuracy/loss
                    ########################################################
                    if validation_count == args.validation_interval:
                        validation_count = 0

                        ####################################################
                        # save the model
                        ####################################################
                        logging.info('SAVING MODEL to disk')
                        t1 = timeit.default_timer() 
                        checkpoint_path = os.path.join(args.checkpoint_dir, 'tf_checkpoint-epoch{}-batch-minibatch{}'.format(epoch, train_minibatch_count-1))
                        saver.save(sess, checkpoint_path)

                        model_weights_path = os.path.join(args.checkpoint_dir, 'model_weights-epoch{}-step{}.npz'.format(epoch, train_minibatch_count-1))
                        model.export_variables(model_weights_path)

                        t2 = timeit.default_timer()
                        logging.info('SAVING MODEL done! (save model to disk): {})'.format(t2 - t1))
                        saver.restore(sess, checkpoint_path)

                        ####################################################
                        # compute validation accuracy/loss
                        ####################################################
                        logging.debug('begin validation accuracy computation')
                        t1 = timeit.default_timer() 

                        cum_validation_loss = 0
                        cum_validation_correct_predictions_l = []
                        cum_validation_correct_predictions3_l = []
                        cum_validation_correct_predictions5_l = []

                        validation_batch_f = load_videos(pool, args.data_root, validation_labels, validation_video_paths[0:args.prefetch_batch_size], args.video_width, args.video_height, args.video_length, args.video_downsample_ratio, False, args.prefetch_batch_size, shared_mem, shared_mem_idx, is_ucf101=args.ucf101, is_imagenet=args.imagenet, is_zipped=args.imagenet_zipped)
                        validation_batch = validation_batch_f()
                        # print('min: {} max:{}'.format(0, num_validation_videos))
                        # print('loaded {} to {}'.format(0, args.prefetch_batch_size))
                         
                        for valid_base_idx in range(0, num_validation_videos, args.prefetch_batch_size):

                            # prefetch the batch ahead of the index (therefore, we must skip loading on the last loop iteration to avoid array overrun)
                            if valid_base_idx+args.prefetch_batch_size < num_validation_videos:
                                next_shared_mem_idx = (shared_mem_idx + 1) % 2
                                validation_batch_f = load_videos(pool, args.data_root, validation_labels, validation_video_paths[valid_base_idx+args.prefetch_batch_size:valid_base_idx+2*args.prefetch_batch_size], args.video_width, args.video_height, args.video_length, args.video_downsample_ratio, False, args.prefetch_batch_size, shared_mem, next_shared_mem_idx, is_ucf101=args.ucf101, is_imagenet=args.imagenet, is_zipped=args.imagenet_zipped)
                                # print('loaded {} to {}'.format(valid_base_idx+args.prefetch_batch_size, valid_base_idx+2*args.prefetch_batch_size))

                            # print('out a video to make sure things are working')
                            # print(validation_batch['video_labelnums'][0], labels_num2str[validation_batch['video_labelnums'][0]])
                            # print(np.min(validation_batch['video_rawframes'][0,0,:,:]), np.max(validation_batch['video_rawframes'][0,0,:,:]))
                            # print(validation_batch['video_rawframes'][0,0,:,:].dtype)
                            # for i in range(args.video_length):
                            #     frame = (np.clip(validation_batch['video_rawframes'][0,i,:,:] + 116, 0, 255)).astype(dtype=np.uint8)
                            #     im = Image.fromarray(frame)
                            #     im.save('/home/jemmons/frame{}.jpg'.format(i))
                            # return 

                            for i in range(0, num_batch_videos, args.train_batch_size):
                                begin = i
                                end = i + args.train_batch_size

                                _step, learn_rate, total_loss_val, predict_vals = sess.run([step, learning_rate, total_loss, predictions],
                                                                                           feed_dict={x_input : validation_batch['video_rawframes'][begin:end,:,:,:],
                                                                                                      x_length : validation_batch['video_numframes'][begin:end],
                                                                                                      y_label : keras.utils.to_categorical(validation_batch['video_labelnums'][begin:end],len(labels_num2str)),
                                                                                                      y_zeros : zeros})


                                assert(not np.isnan(total_loss_val))
                                assert(not np.any(np.isnan(predict_vals)))

                                analysis = TrainingLogger.process_prediction(predict_vals, validation_batch['video_labelnums'][begin:end], validation_batch['video_numframes'][begin:end])
                                correct_predictions = analysis[0]['correct_predictions']
                                correct_predictions3 = analysis[0]['correct_predictions3']
                                correct_predictions5 = analysis[0]['correct_predictions5']

                                if len(cum_validation_correct_predictions_l) == 0:
                                    for feedback_idx in range(_predict_vals.shape[1]):
                                        cum_validation_correct_predictions_l.append([])
                                        cum_validation_correct_predictions3_l.append([])
                                        cum_validation_correct_predictions5_l.append([])

                                for feedback_idx in range(_predict_vals.shape[1]):
                                    correct_predictions = analysis[feedback_idx]['correct_predictions']
                                    correct_predictions3 = analysis[feedback_idx]['correct_predictions3']
                                    correct_predictions5 = analysis[feedback_idx]['correct_predictions5']

                                    cum_validation_correct_predictions_l[feedback_idx].append(correct_predictions)
                                    cum_validation_correct_predictions3_l[feedback_idx].append(correct_predictions3)
                                    cum_validation_correct_predictions5_l[feedback_idx].append(correct_predictions5)

                                    logging.debug('VALIDATION batch accuracy (top-1) (feedback:{}): {}'.format(feedback_idx, correct_predictions / args.train_batch_size))
                                    logging.debug('VALIDATION batch accuracy (top-3) (feedback:{}): {}'.format(feedback_idx, correct_predictions3 / args.train_batch_size))
                                    logging.debug('VALIDATION batch accuracy (top-5) (feedback:{}): {}'.format(feedback_idx, correct_predictions5 / args.train_batch_size))

                                logging.debug('VALIDATION batch loss: {}'.format(total_loss_val))
                                logging.debug('begin load videos complete-future')
                                t1 = timeit.default_timer() 
                                validation_batch = validation_batch_f()                        
                                shared_mem_idx = next_shared_mem_idx
                                t2 = timeit.default_timer()
                                logging.debug('done! (load videos complete-future): {})'.format(t2-t1))

                                cum_validation_loss += _total_loss_val

                        for feedback_idx in range(len(cum_validation_correct_predictions_l)):
                            cum_validation_correct_predictions = sum(cum_validation_correct_predictions_l[feedback_idx])
                            cum_validation_correct_predictions3 = sum(cum_validation_correct_predictions3_l[feedback_idx])
                            cum_validation_correct_predictions5 = sum(cum_validation_correct_predictions5_l[feedback_idx])
                            
                            logfile.write('{}, {}, {}, {}, {}, , , , , {}, {}, {}, {}, {}, {}\n'.format(train_batch_count, epoch, _step, learn_rate, feedback_idx, cum_validation_correct_predictions/num_validation_videos, cum_validation_correct_predictions3/num_validation_videos, cum_validation_correct_predictions5/num_validation_videos, cum_validation_loss, checkpoint_path, model_weights_path))

                            logging.info('VALIDATION TOTAL\taccuracy (top-1) (feedback:{}): {}'.format(feedback_idx, cum_validation_correct_predictions / num_validation_videos))
                            logging.info('VALIDATION TOTAL\taccuracy (top-3) (feedback:{}): {}'.format(feedback_idx, cum_validation_correct_predictions3 / num_validation_videos))
                            logging.info('VALIDATION TOTAL\taccuracy (top-5) (feedback:{}): {}'.format(feedback_idx, cum_validation_correct_predictions5 / num_validation_videos))
                            logging.info('VALIDATION TOTAL\tloss: {}'.format(cum_validation_loss))
                    
                        #logfile.flush()
                        logging.info('VALIDATION TOTAL\tnum_examples: {}'.format(num_validation_videos))
                        

                        t2 = timeit.default_timer()
                        logging.debug('done! (validation accuracy computation): {})'.format(t2 - t1))

                        ####################################################
                        # restore model to the state prior to computing validation accuracy/loss
                        ####################################################
                        saver.restore(sess, checkpoint_path)
                        logfile.flush()

                    validation_count += 1
                    train_batch_count += 1

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='train the feedbacknet for the UCF-101 data set')

    # dataset parameters (all required)
    parser.add_argument('label_file', type=str, nargs=None,
                        help='labels file')

    parser.add_argument('validation_file', type=str, nargs=None,
                        help='validation label file')

    parser.add_argument('data_file', type=str, nargs=None,
                        help='data label file')

    parser.add_argument('data_root', type=str, nargs=None,
                        help='root of dataset')

    parser.add_argument('model_name', type=str, nargs=None,
                        help='name of the model to train')

    parser.add_argument('checkpoint_dir', type=str, nargs=None,
                        help='location to store checkpoints during training')

    parser.add_argument('train_progress_csv', type=str, nargs=None,
                        help='csv file to record the training and validation error/loss')

    parser.add_argument('--ucf101',
                        help='flag specifying the dataset being used',
                        action='store_true',
                        default=False)

    # parser.add_argument('--twentybn',
    #                     help='flag specifying the dataset being used (legacy, used to refer to the jester dataset)',
    #                     action='store_true',
    #                     default=False)
    
    parser.add_argument('--jester',
                        help='flag specifying the dataset being used',
                        action='store_true',
                        default=False)

    parser.add_argument('--something_something',
                        help='flag specifying the dataset being used',
                        action='store_true',
                        default=False)

    parser.add_argument('--imagenet',
                        help='flag specifying the dataset being used',
                        action='store_true',
                        default=False)

    parser.add_argument('--imagenet_zipped',
                        help='flag specifying the dataset being used',
                        action='store_true',
                        default=False)

    # tuning parameters (optional)
    parser.add_argument('--pretrain_root_prefix', type=str, nargs=None,
                        help='the path prefix to where the pretrained model live',
                        default='/home/jemmons/data')

    parser.add_argument('--num_cpus', type=int, nargs=None,
                        help='number of cpus to use during training', 
                        default=6)

    parser.add_argument('--num_gpus', type=int, nargs=None,
                        help='number of gpus to use during training')

    parser.add_argument('--overfit',
                        help='overfit on a small chunk [16 data points] of data (official run: False)',
                        action='store_true',
                        default=False)

    parser.add_argument('--overfitt',
                        help='overfit on a larger chunk of data (official run: False)',
                        action='store_true',
                        default=False)

    parser.add_argument('--fine_tune_vgg',
                        help='fine tune the pretrained vgg16 network',
                        action='store_true',
                        default=False)

    parser.add_argument('--continue_training',
                        help='load the most recent set of weights and continue training (official run: False)',
                        action='store_true',
                        default=False)

    parser.add_argument('--video_width', type=int, nargs=None,
                        help='the width to rescale all videos (official run: 112)',
                        default=112)

    parser.add_argument('--video_height', type=int, nargs=None,
                        help='the height to rescale all videos (official run: 112)',
                        default=112)

    parser.add_argument('--video_length', type=int, nargs=None,
                        help='the num frames to truncate all videos (official run: 20)',
                        default=20)
    
    parser.add_argument('--video_downsample_ratio', type=int, nargs=None,
                        help='the temporal downsampling ratio to use all the videos (official run: 5 for ucf-101, 2 for 20bn-jester)',
                        default=5)
    
    parser.add_argument('--num_epoch', type=int, nargs=None,
                        help='number of training epochs over the whole dataset (official run: 1024)',
                        default=1024)

    parser.add_argument('--last_loss_multipler', type=float, nargs=None,
                        help='the multipler to apply to the loss on the last frame of the video (official run: 1)',
                        default=1)

    parser.add_argument('--learning_rate_init', type=float, nargs=None,
                        help='initial vlaue for learning rate for the optimization (official run: 0.1)',
                        default=1e-1)

    parser.add_argument('--learning_rate_decay', type=float, nargs=None,
                        help='learning rate decay (per prefetch_batch) for the optimization (official run: XXXX)',
                        default=0.995)

    parser.add_argument('--learning_rate_min', type=float, nargs=None,
                        help='minimum learning rate for the optimization (official run: 1e-4)',
                        default=1e-4)

    parser.add_argument('--global_step_init', type=float, nargs=None,
                        help='the initial global training step count (official run: None)',
                        default=None)
    
    parser.add_argument('--train_batch_size', type=int, nargs=None,
                        help='training batch size; num videos per batch (official run: 32)',
                        default=32)

    parser.add_argument('--prefetch_batch_size', type=int, nargs=None,
                        help='number of videos to prefetch (official run: 1024)',
                        default=1024)

    parser.add_argument('--validation_interval', type=int, nargs=None,
                        help='number of training batches to process before printing validation error (official run: 32)',
                        default=32)

    
    args = parser.parse_args()
    main(args)

