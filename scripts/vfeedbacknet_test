#!/usr/bin/env python3

import asyncio
import argparse
import logging
import csv
import os
import sys
import timeit
import random

import numpy as np
import multiprocessing as mp
import sharedmem as sm

import keras
import tensorflow as tf
from tensorflow.python.client import device_lib

import vfeedbacknet as v
from vfeedbacknet import TrainingLogger, ModelLogger
from vfeedbacknet import pool_init, prepare_video, load_videos 

#logging.basicConfig(level=logging.INFO)
logging.basicConfig(level=logging.INFO)

def main(args):
    logging.info('loading input files')
    assert args.ucf101 ^ args.jester ^ args.something_something, 'you must specify exactly one dataset to use for testing (XOR)'

    round2num = lambda x, num: num * (x // num)

    with open(args.label_file, 'r') as csvfile:
        labels_num2str = None
        labels_str2num = None
        
        if args.ucf101:
            reader = csv.reader(csvfile, delimiter=' ')
            labels_num2str = [ item[1] for item in reader ]
            labels_str2num =  { label : idx  for idx,label in zip(range(len(labels_num2str)), labels_num2str) }
        elif args.jester or args.something_something:
            reader = csv.reader(csvfile, delimiter=';')
            labels_num2str = [ item[0] for item in reader ]
            labels_str2num =  { label : idx  for idx,label in zip(range(len(labels_num2str)), labels_num2str) }
            
    with open(args.data_file, 'r') as csvfile:
        data_video_paths = None
        data_labels = None

        if args.ucf101:
            reader = csv.reader(csvfile, delimiter=' ')
            data_video_paths = [ item[0].split('.')[0] for item in reader ] 
            data_labels = { item : labels_str2num[item.split('/')[0]] for item in data_video_paths } 
        elif (args.jester or args.something_something) and not args.competition:
            reader = csv.reader(csvfile, delimiter=';')
            data_labels = {}
            for item in reader:
                basename = str(int(item[0]))
                dirname = basename[-1]+'.dir'
                video_path = os.path.join(dirname, basename)
                data_labels[video_path] = labels_str2num[item[1].lower()]
            data_video_paths = list(data_labels.keys())
        elif args.jester and args.competition:
            reader = csv.reader(csvfile, delimiter=';')
            paths = [int(item[0]) for item in reader]
        elif args.something_something and args.competition:
            raise NotImplementedError('args.something_something an args.competition cannot be set at the same time yet')
            
            padded_len = round2num(len(paths), args.prefetch_batch_size) + args.prefetch_batch_size
            # print(len(paths))
            # print(padded_len)
            # print(padded_len / args.prefetch_batch_size)
            
            pad = [ paths[0] for _ in range(padded_len - len(paths)) ]
            paths += pad

            # print(len(paths))
            
            data_video_paths = np.asarray( paths )
            data_labels = { data_video_paths[i] : 0 for i in range(len(data_video_paths)) }
            #data_labels = { int(item[0]) : labels_str2num[item[1].lower()] for item in reader }

            # print(len(data_video_paths))
            # print(len(list(data_labels.keys())))
            
        random.seed(123) # shuffle with this particular seed to help with debugging
        if not args.competition:
            random.shuffle(data_video_paths)

        if args.overfit:
            logging.info('OVERFITTING!')
            args.prefetch_batch_size = 16
            data_video_paths = data_video_paths[:args.prefetch_batch_size] # force overfitting
        elif args.overfitt:
            logging.info('OVERFITTING!')
            args.prefetch_batch_size = 1024
            data_video_paths = data_video_paths[:args.prefetch_batch_size] # force overfitting

    # allocate shared memory up front
    logging.info('allocating memory')
    logging.debug('begin allocate memory buffers')

    t1 = timeit.default_timer()
    shared_mem = [sm.empty(args.prefetch_batch_size*args.video_length*args.video_height*args.video_width*3, dtype='f')]
    zeros = np.zeros((args.eval_batch_size,))
    t2 = timeit.default_timer()
    logging.debug('done! (allocate memory buffers: {})'.format(t2-t1))

    num_data_videos = round2num(len(data_video_paths), args.prefetch_batch_size)
    num_batch_videos = round2num(args.prefetch_batch_size, args.eval_batch_size)
    assert(num_batch_videos == args.prefetch_batch_size) # make `prefetch_batch_size` a multiple of `eval_batch_size`

    # get batch of data
    logging.info('begin processing data')
    with mp.Pool(processes=mp.cpu_count(), initializer=pool_init, initargs=(shared_mem,)) as pool:
        video_batch = None
        validation_batch_count = 0

        with tf.Session() as sess:
            logging.info('load model')
            saver = tf.train.import_meta_graph(args.checkpoint_prefix+'.meta')
            saver.restore(sess, args.checkpoint_prefix)

            # graph = tf.get_default_graph()    
            # for op in graph.get_operations():
            #     print((op.name))

            logging.info('process data')
            cum_validation_correct_predictions_l = []
            cum_validation_correct_predictions3_l = []
            cum_validation_correct_predictions5_l = []
            cum_validation_loss = 0

            for batch_base_idx in range(0, num_data_videos, args.prefetch_batch_size):
                video_paths_batch = data_video_paths[batch_base_idx:batch_base_idx+args.prefetch_batch_size]
                validation_batch_count += 1

                logging.debug('begin load videos)')
                t1 = timeit.default_timer()
                validation_batch_f = load_videos(pool, args.data_root, data_labels, video_paths_batch, args.video_width, args.video_height, args.video_length, args.video_downsample_ratio, False, args.prefetch_batch_size, shared_mem, 0)
                validation_batch = validation_batch_f()

                # hack to get Y value from rgb frames
                # rgb = (validation_batch['video_rawframes'] * 128) + 116
                # y = ((0.299*rgb[:,:,:,:,0] + 0.587*rgb[:,:,:,:,1] + 0.144*rgb[:,:,:,:,2]).astype(np.float32) - 116) / 128
                # validation_batch['video_rawframes'] = y

                t2 = timeit.default_timer()
                logging.debug('done! (load videos): {})'.format(t2-t1))

                # print('out a video to make sure things are working')
                # for i in range(40):
                #     frame = (128*(validation_batch['video_rawframes'][0,i,:,:,:] + 1)).astype(dtype=np.uint8)
                #     im = Image.fromarray(frame)
                #     im.save('/home/jemmons/frame{}.jpg'.format(i))

                assert(not np.any(np.isnan(validation_batch['video_rawframes'])))

                ########################################################
                # evaluate model
                ########################################################
                logging.debug('begin test batch')
                t1 = timeit.default_timer()
                for i in range(0, num_batch_videos, args.eval_batch_size):
                    begin = i
                    end = i + args.eval_batch_size
                    batch_labels = keras.utils.to_categorical(validation_batch['video_labelnums'][begin:end], len(labels_num2str))

                    #_total_loss_val, _loss_vals, _predict_vals = sess.run(['total_loss:0', 'loss:0', 'predictions:0'],
                    _total_loss_val, _loss_vals, _predict_vals = sess.run(['total_loss:0', 'losses_agg:0', 'predictions_agg:0'],
                                                                          feed_dict={'x_input:0' : validation_batch['video_rawframes'][begin:end,:,:,:],
                                                                                     'x_length:0' : validation_batch['video_numframes'][begin:end],
                                                                                     'y_label:0' : batch_labels,
                                                                                     'y_zeros:0' : zeros})

                    assert(not np.isnan(_total_loss_val))
                    assert(not np.any(np.isnan(_predict_vals)))

                    
                    analysis = TrainingLogger.process_prediction(_predict_vals,
                                                                 validation_batch['video_labelnums'][begin:end],
                                                                 validation_batch['video_numframes'][begin:end],
                                                                 competition=args.competition,
                                                                 competition_video_num=video_paths_batch[begin:end],
                                                                 competition_labels=labels_num2str)

                    correct_predictions = analysis[0]['correct_predictions']
                    correct_predictions3 = analysis[0]['correct_predictions3']
                    correct_predictions5 = analysis[0]['correct_predictions5']

                    if len(cum_validation_correct_predictions_l) == 0:
                        for feedback_idx in range(_predict_vals.shape[1]):
                            cum_validation_correct_predictions_l.append([])
                            cum_validation_correct_predictions3_l.append([])
                            cum_validation_correct_predictions5_l.append([])

                    for feedback_idx in range(_predict_vals.shape[1]):
                        correct_predictions = analysis[feedback_idx]['correct_predictions']
                        correct_predictions3 = analysis[feedback_idx]['correct_predictions3']
                        correct_predictions5 = analysis[feedback_idx]['correct_predictions5']

                        cum_validation_correct_predictions_l[feedback_idx].append(correct_predictions)
                        cum_validation_correct_predictions3_l[feedback_idx].append(correct_predictions3)
                        cum_validation_correct_predictions5_l[feedback_idx].append(correct_predictions5)

                        logging.debug('VALIDATION batch accuracy (top-1) (feedback:{}): {}'.format(feedback_idx, correct_predictions / args.eval_batch_size))
                        logging.debug('VALIDATION batch accuracy (top-3) (feedback:{}): {}'.format(feedback_idx, correct_predictions3 / args.eval_batch_size))
                        logging.debug('VALIDATION batch accuracy (top-5) (feedback:{}): {}'.format(feedback_idx, correct_predictions5 / args.eval_batch_size))

                    logging.debug('VALIDATION batch loss: {}'.format(_total_loss_val))

                    cum_validation_loss += _total_loss_val
                    validation_batch_count += 1
                t2 = timeit.default_timer()
                
        # print out the final accuracy and loss
        for feedback_idx in range(len(cum_validation_correct_predictions_l)):
            cum_validation_correct_predictions = sum(cum_validation_correct_predictions_l[feedback_idx])
            cum_validation_correct_predictions3 = sum(cum_validation_correct_predictions3_l[feedback_idx])
            cum_validation_correct_predictions5 = sum(cum_validation_correct_predictions5_l[feedback_idx])

            logging.info('VALIDATION TOTAL\taccuracy (top-1) (feedback:{}): {}'.format(feedback_idx, cum_validation_correct_predictions / num_data_videos))
            logging.info('VALIDATION TOTAL\taccuracy (top-3) (feedback:{}): {}'.format(feedback_idx, cum_validation_correct_predictions3 / num_data_videos))
            logging.info('VALIDATION TOTAL\taccuracy (top-5) (feedback:{}): {}'.format(feedback_idx, cum_validation_correct_predictions5 / num_data_videos))
            logging.info('VALIDATION TOTAL\tloss: {}'.format(cum_validation_loss))

        sys.stdout.flush()

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='test the feedbacknet for the UCF-101 data set')

    # dataset parameters
    parser.add_argument('label_file', type=str, nargs=None,
                        help='labels file')

    parser.add_argument('data_file', type=str, nargs=None,
                        help='data label file')

    parser.add_argument('data_root', type=str, nargs=None,
                        help='root of UCF-101 dataset)')

    parser.add_argument('checkpoint_prefix', type=str, nargs=None,
                        help='path prefix to the model {metadata,data} to load')

    parser.add_argument('--raw_predictions', type=str, nargs=None,
                        help='the path to the .npz file to store raw predictions')

    parser.add_argument('--ucf101',
                        help='flag specifying the dataset being used',
                        action='store_true',
                        default=False)

    # parser.add_argument('--twentybn',
    #                     help='flag specifying the dataset being used (legacy; used to referred to jester dataset)',
    #                     action='store_true',
    #                     default=False)

    parser.add_argument('--jester',
                        help='flag specifying the dataset being used',
                        action='store_true',
                        default=False)

    parser.add_argument('--something_something',
                        help='flag specifying the dataset being used',
                        action='store_true',
                        default=False)

    parser.add_argument('--competition',
                        help='output for 20bn competition',
                        action='store_true',
                        default=False)

    # tuning parameters
    parser.add_argument('--overfit',
                        help='overfit on a small chunk [64 data points] of data (official run: False)',
                        action='store_true',
                        default=False)

    parser.add_argument('--overfitt',
                        help='overfit on a larger chunk of data (official run: False)',
                        action='store_true',
                        default=False)

    parser.add_argument('--video_width', type=int, nargs=None,
                        help='the width to rescale all videos (official run: XXX for UCF-101)',
                        default=112)

    parser.add_argument('--video_height', type=int, nargs=None,
                        help='the height to rescale all videos (official run: XXXX for UCF-101)',
                        default=112)

    parser.add_argument('--video_length', type=int, nargs=None,
                        help='the num frames to truncate all videos (official run: XXXX for UCF-101)',
                        default=20)

    parser.add_argument('--video_downsample_ratio', type=int, nargs=None,
                        help='the num frames to truncate all videos (official run: XXXX for UCF-101)',
                        default=5)
    
    parser.add_argument('--eval_batch_size', type=int, nargs=None,
                        help='number of videos to eval at a time (official run: XXXX for UCF-101)',
                        default=16)

    parser.add_argument('--prefetch_batch_size', type=int, nargs=None,
                        help='number of videos to prefetch (official run: XXXX for UCF-101)',
                        default=1024)

    args = parser.parse_args()
    main(args)

